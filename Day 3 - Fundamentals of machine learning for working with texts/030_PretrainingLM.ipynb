{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.0 Pretraining Language Models\n",
    "\n",
    "There are many pretrained BERT models that can be used \"off-the-shelf\".  However, there are times when it is advantageous to train or fine-tune a new language model for downstream NLP tasks.  For example, medical papers use vocabularies that are specific to the medical domain, so a language model trained on medical papers will be better suited to projects that process medical text than one trained on more general text.  \n",
    "\n",
    "In this notebook, you'll learn how to pretrain a BERT language model with domain-specific data.  \n",
    "    \n",
    "**[3.1 Data Preparation](#3.1-Data-Preparation)<br>**\n",
    "**[3.2 Training the BERT Tokenizer](#3.2-Training-the-BERT-Tokenizer)<br>**\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[3.2.1 Exercise: Tokenize a Term](#3.2.1-Exercise:-Tokenize-a-Term)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[3.2.2 Update the BERT Vocabulary](#3.2.2-Update-the-BERT-Vocabulary)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[3.2.3 Exercise: Train a Larger Vocabulary](#3.2.3-Exercise:-Train-a-Larger-Vocabulary)<br>\n",
    "**[3.3 Launch BERT Pretraining with NeMo](#3.3-Launch-BERT-Pretraining-with-NeMo)<br>**\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[3.3.1 TensorBoard Visualization](#3.3.1-TensorBoard-Visualization)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[3.3.2 Practical Considerations](#3.3.2-Practical-Considerations)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3.1 Data Preparation\n",
    "\n",
    "Masked neural language models, such as BERT, are trained on text.  However, the text must first be transformed into numerical representations, a process called tokenization.  The network is then trained by masking random words in the input sentence and predicting the missing words.  The trained language model can then be used in downstream NLP tasks, where it is referred to as a \"pretrained\" language model.\n",
    "\n",
    "With NVIDIA NeMo, the tokenization can be done either on-the-fly during training or offline before training.\n",
    "\n",
    "- **On-the-fly data preprocessing:** The training and validation text files should have words separated by spaces:\n",
    "                                [WORD] [SPACE] [WORD] [SPACE] [WORD] [SPACE] [WORD]\n",
    "                                \n",
    "- **Offline data preprocessing:** Data is prepared in advance in HD5F format. This is the recommended preprocessing for large text corpora.  Refer to [BERT quick start guide](https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/LanguageModeling/BERT#quick-start-guide) for the offline data preprocessing script. \n",
    "\n",
    "In our example, we will use the on-the-fly data preprocessing pipeline.  We'll train BERT on the [NCBI-disease corpus](https://www.ncbi.nlm.nih.gov/CBBresearch/Dogan/DISEASE/).\n",
    "The NCBI corpus is a set of 793 PubMed abstracts.  Our goal is to create a pretrained model for the medical domain.  Here's an example of text abstracts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low levels of beta hexosaminidase A in healthy individuals with apparent deficiency of this enzyme. Appreciable beta hexosaminidase A ( hex A ) activity has been detected in cultured skin fibroblasts and melanoma tissue from healthy individuals previously reported as having deficiency of hex A activity indistinguishable from that of patients with Tay-Sachs disease ( TSD ) . Identification and quantitation of hex A , amounting to 3 . 5 % -6 . 9 % of total beta hexosaminidase activity , has been obtained by cellulose acetate gel electrophoresis , DEAE-cellulose ion-exchange chromatography , radial immunodiffusion , and radioimmunoassay . Previous family studies suggested that these individuals may be compound heterozygotes for the common mutant TSD gene and a rare ( allelic ) mutant gene . Thus , the postulated rate mutant gene appears to code for the expression of low amounts of hex A . Heterozygotes for the rare mutant may be indistinguishable from heterozygotes for the common TSD mutant . However , direct visualization and quantitation of hex A by the methods described may prevent false-positive prenatal diagnosis of TSD in fetuses having the incomplete hex A deficiency of the type described in the four healthy individuals \n",
      "The RB1 gene mutation in a child with ectopic intracranial retinoblastoma. The RB1 gene mutation was investigated in a child with ectopic intracranial retinoblastoma using DNA obtained from both the pineal and retinal tumours of the patient . A nonsense mutation in exon 17 ( codon 556 ) of the RB1 gene was found to be present homozygously in both the retinal and the pineal tumours . The same mutation was present heterozygously in the DNA from the constitutional cells of the patient , proving it to be of germline origin . The initial mutation was shown to have occurred in the paternally derived RB1 allele . The mutation is in an area of the gene that encodes the protein-binding region known as the pocket region and has been detected in other cases of retinoblastoma . . \n",
      "Paternal transmission of congenital myotonic dystrophy. We report a rare case of paternally transmitted congenital myotonic dystrophy ( DM ) . The proband is a 23 year old , mentally retarded male who suffers severe muscular weakness . He presented with respiratory and feeding difficulties at birth . His two sibs suffer from childhood onset DM . Their late father had the adult type of DM , with onset around 30 years . Only six other cases of paternal transmission of congenital DM have been reported recently . We review the sex related effects on transmission of congenital DM . Decreased fertility of males with adult onset DM and contraction of the repeat upon male transmission contribute to the almost absent occurrence of paternal transmission of congenital DM . Also the fathers of the reported congenitally affected children showed , on average , shorter CTG repeat lengths and hence less severe clinical symptoms than the mothers of children with congenital DM . We conclude that paternal transmission of congenital DM is rare and preferentially occurs with onset of DM past 30 years in the father . . \n",
      "Low frequency of BRCA1 germline mutations in 45 German breast/ovarian cancer families. In this study we investigated 45 German breast / ovarian cancer families for germline mutations in the BRCA1 gene . We identified four germline mutations in three breast cancer families and in one breast-ovarian cancer family . among these were one frameshift mutation , one nonsense mutation , one novel splice site mutation , and one missense mutation . The missense mutation was also found in 2 . 8 % of the general population , suggesting that it is not disease associated . The average age of disease onset in those families harbouring causative mutations was between 32 . 3 and 37 . 4 years , whereas the family harbouring the missense mutation had an average age of onset of 51 . 2 years . These findings show that BRCA1 is implicated in a small fraction of breast / ovarian cancer families suggesting the involvement of another susceptibility gene ( s ) \n",
      "A novel common missense mutation G301C in the N-acetylgalactosamine-6-sulfate sulfatase gene in mucopolysaccharidosis IVA. Mucopolysaccharidosis IVA ( MPS IVA ) is an autosomal recessive lysosomal storage disorder caused by a genetic defect in N-acetylgalactosamine-6-sulfate sulfatase ( GALNS ) . In previous studies , we have found two common mutations in Caucasians and Japanese , respectively . To characterize the mutational spectrum in various ethnic groups , mutations in the GALNS gene in Colombian MPS IVA patients were investigated , and genetic backgrounds were extensively analyzed to identify racial origin , based on mitochondrial DNA ( mtDNA ) lineages . Three novel missense mutations never identified previously in other populations and found in 16 out of 19 Colombian MPS IVA unrelated alleles account for 84 . 2 % of the alleles in this study . The G301C and S162F mutations account for 68 . 4 % and 10 . 5 % of mutations , respectively , whereas the remaining F69V is limited to a single allele . The skewed prevalence of G301C in only Colombian patients and haplotype analysis by restriction fragment length polymorphisms in the GALNS gene suggest that G301C originated from a common ancestor . Investigation of the genetic background by means of mtDNA lineages indicate that all our patients are probably of native American descent \n"
     ]
    }
   ],
   "source": [
    "! tail -5 /dli/task/data/train.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3.2 Training the BERT Tokenizer\n",
    "\n",
    "As discussed in the previous notebook, the BERT tokenizer splits the text into tokens following a predefined vocabulary. The tokenizer algorithm generates the vocabulary following variants of Top-K frequent words from text corpus.\n",
    "\n",
    "The vocabulary size is limited because the training cost increases with the size of the vocabulary. Including all unique words from the text corpus into the vocabulary would explode the complexity of training beyond the capabilities of the tokenizer. For instance, the BERT model that was released in 2018, with a subword tokenizer algorithm called WordPiece, has a vocabulary limit of 30,000.\n",
    "\n",
    "How, then, do tokenizers deal with terms that are not part of the vocabulary, or **out-of-vocabulary (OOV)** words?\n",
    "\n",
    "1. One option is to replace OOV words with a special token \\[UNK\\]. In this case, all OOV terms will have the same representation for the neural network loosing the semantic. \n",
    "1. A second option is to split OOV words at the character level. This increases the size of the input to the neural language model, adding the challenge of learning the relationship between characters to keep the semantic.\n",
    "1. Sub-word tokenizers, such as BERT WordPiece, provide a solution in between the word token and character split option. It tokenizes OOV words into subwords.\n",
    "\n",
    "Let's have a look at the `bert-base-uncased` tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75a54fc928734fe090aee207b0eddb62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06de677502164eb49fdd784c4da8b70b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ea80d3135ac4943b693e0165b1b4671",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e948460bc6f4126a3e0f388f9e2332e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bos_token, but it is not set yet.\n",
      "Using eos_token, but it is not set yet.\n"
     ]
    }
   ],
   "source": [
    "# import nemo nlp collection \n",
    "from nemo.collections import nlp as nemo_nlp\n",
    "\n",
    "# load the bert-base-uncased tokenizer \n",
    "tokenizer_uncased = nemo_nlp.modules.get_tokenizer(tokenizer_name=\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The vocabulary size:  30522\n"
     ]
    }
   ],
   "source": [
    "# get the vocabulary size\n",
    "print(\" The vocabulary size: \", tokenizer_uncased.vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, take a look at the format tokenization for years with BERT. Years prior to 2021 appear frequently enough in the corpus to be part of the vocabulary, while years in the future are OOV and are split into sub-tokens.\n",
    "\n",
    "Try it in the cell below using the `tokenizer_uncased.text_to_tokens()` function for various years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized year:  ['2019']\n",
      "Tokenized year:  ['2020']\n",
      "Tokenized year:  ['2021']\n",
      "Tokenized year:  ['202', '##2']\n",
      "Tokenized year:  ['202', '##3']\n",
      "Tokenized year:  ['203', '##0']\n"
     ]
    }
   ],
   "source": [
    "# Bert tokenizer for years\n",
    "print(\"Tokenized year: \", tokenizer_uncased.text_to_tokens('2019'))\n",
    "print(\"Tokenized year: \", tokenizer_uncased.text_to_tokens('2020'))\n",
    "print(\"Tokenized year: \", tokenizer_uncased.text_to_tokens('2021'))\n",
    "print(\"Tokenized year: \", tokenizer_uncased.text_to_tokens('2022'))\n",
    "print(\"Tokenized year: \", tokenizer_uncased.text_to_tokens('2023'))\n",
    "print(\"Tokenized year: \", tokenizer_uncased.text_to_tokens('2030'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The years tokenization example gives us some intuition into the process.  How about domain-specific context such as medical jargon? For a concrete example, try again with the following sentence:\n",
    "\n",
    "_\"Further studies suggested that low dilutions of C5D serum contain a factor or factors interfering at some step in the hemolytic assay of C5 rather than a true C5 inhibitor or inactivator\"_\n",
    "\n",
    "This sentence includes several medical terms such as dilutions, C5D, C5, hemolytic and assay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized sentence:  ['further', 'studies', 'suggested', 'that', 'low', 'dil', '##ution', '##s', 'of', 'c', '##5', '##d', 'serum', 'contain', 'a', 'factor', 'or', 'factors', 'interfering', 'at', 'some', 'step', 'in', 'the', 'hem', '##ol', '##ytic', 'ass', '##ay', 'of', 'c', '##5', 'rather', 'than', 'a', 'true', 'c', '##5', 'inhibitor', '.']\n"
     ]
    }
   ],
   "source": [
    "# Bert tokenizer for domain-specific example\n",
    "SAMPLES = \"Further studies suggested that low dilutions of C5D serum contain a factor or factors interfering at some step in the hemolytic assay of C5 rather than a true C5 inhibitor.\"\n",
    "print(\"Tokenized sentence: \", tokenizer_uncased.text_to_tokens(SAMPLES))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see medical jargon tokenized as subwords: \n",
    "- dilutions -> 'dil', '##ution', '##s'\n",
    "- hemolytic ->'hem', '##ol', '##ytic'\n",
    "- assay -> 'ass', '##ay'\n",
    "- C5 ->'c', '##5'\n",
    "- C5D ->'c', '##5', '##d'\n",
    "\n",
    "The medical jargon such as dilutions, hemolytic and assay are not in the standard BERT tokenizer vocabulary. Therefore, they cannot be individually tokenized and are divided into subwords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.1 Exercise: Tokenize a Term\n",
    "Correct the \"FIXME\" lines below to tokenize the term \"COVID-19\" using the BERT tokenizer.  Check the [solution](solutions/ex3.2.1.ipynb) if you need to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized sentence:  ['co', '##vid', '-', '19']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize a new term\n",
    "TEXT = 'COVID-19'\n",
    "print(\"Tokenized sentence: \", tokenizer_uncased.text_to_tokens(TEXT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.2 Update the BERT Vocabulary\n",
    "\n",
    "It is possible to add domain specific words into the tokenizer vocabulary with the `tokenizer_uncased.tokenizer.add_tokens()` function. The embeddings vector for each new token will be initialized with random values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The vocabulary size before:  30524\n",
      " The vocabulary size after :  30524\n"
     ]
    }
   ],
   "source": [
    "# Add some medical jargon to the vocabulary of Bert tokenizer\n",
    "additional_tokens = tokenizer_uncased.tokenizer.add_tokens([\"dilutions\", \"hemolytic\"])\n",
    "print(\" The vocabulary size before: \", tokenizer_uncased.vocab_size)\n",
    "print(\" The vocabulary size after : \", tokenizer_uncased.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized sentence:  ['further', 'studies', 'suggested', 'that', 'low', 'dilutions', 'of', 'c', '##5', '##d', 'serum', 'contain', 'a', 'factor', 'or', 'factors', 'interfering', 'at', 'some', 'step', 'in', 'the', 'hemolytic', 'ass', '##ay', 'of', 'c', '##5', 'rather', 'than', 'a', 'true', 'c', '##5', 'inhibitor', '.']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the sentence with the new vocabulary \n",
    "print(\"Tokenized sentence: \", tokenizer_uncased.text_to_tokens(SAMPLES))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the number of domain-specific words to incorporate into the vocabulary is high, it is the best to train a new tokenizer from a domain-specific corpus, rather than to use the pretrained tokenizer. \n",
    "\n",
    "Let's train a new WordPiece tokenizer on the [NCBI-disease corpus] corpus, limiting the vocabulary size to 10,000. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size= 10000\n",
    "text_corpus=[\"/dli/task/data/train.txt\"]\n",
    "\n",
    "# add the special tokens required for BERT pretraining.\n",
    "special_tokens = [\"<PAD>\",\"<UNK>\",\"<CLS>\",\"<SEP>\",\"<MASK>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "my_bert_tokenizer = BertWordPieceTokenizer()\n",
    "my_bert_tokenizer.train(files=text_corpus, vocab_size=vocab_size,\n",
    "                        min_frequency=1, special_tokens=special_tokens,\n",
    "                        show_progress=True, wordpieces_prefix=\"##\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The new vocabulary size  :  10000\n"
     ]
    }
   ],
   "source": [
    "# get the new vocabulary size\n",
    "print(\" The new vocabulary size  : \", len(my_bert_tokenizer.get_vocab()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/dli/task/data/vocab.txt']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the new vocabulary \n",
    "my_bert_tokenizer.save_model(directory=\"/dli/task/data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "electrophysiology\n",
      "d17s857\n",
      "delayed\n",
      "maintaining\n",
      "contributions\n",
      "arg170\n",
      "362arg\n",
      "362ser\n",
      "grandmother\n",
      "grandmatrilineal\n",
      "cytoskeleton\n",
      "tyr231\n",
      "tyr180\n",
      "israelis\n",
      "d14s291\n",
      "angioedema\n",
      "angiokeratoma\n",
      "d13s314\n",
      "d13s316\n",
      "portugal\n"
     ]
    }
   ],
   "source": [
    "!tail -20 /dli/task/data/vocab.txt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the vocabulary is defined, we can load the tokenizer with the new vocabulary using the `nemo_nlp.modules.get_tokenizer()` function. Let's tokenize the previous text sample and compare to the vanilla BERT tokenizer. \n",
    "The domain-specific jargon should now be encoded as individual tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT tokenizer with custom vocabulary:  ['further', 'studies', 'suggested', 'that', 'low', 'dil', '##ution', '##s', 'of', 'c5d', 'serum', 'contain', 'a', 'factor', 'or', 'factors', 'interfer', '##ing', 'at', 'some', 'step', 'in', 'the', 'hemolytic', 'assay', 'of', 'c5', 'rather', 'than', 'a', 'true', 'c5', 'inhibitor', '.']\n"
     ]
    }
   ],
   "source": [
    "# load the tokenizer from the vocabulary \n",
    "special_tokens_dict = {\"unk_token\": \"<UNK>\", \"sep_token\": \"<SEP>\", \"pad_token\": \"<PAD>\", \"bos_token\": \"<CLS>\", \"mask_token\": \"<MASK>\",\"eos_token\": \"<SEP>\", \"cls_token\": \"<CLS>\"}\n",
    "tokenizer_custom = nemo_nlp.modules.get_tokenizer(tokenizer_name=\"bert-base-uncased\", vocab_file='/dli/task/data/vocab.txt', special_tokens=special_tokens_dict)\n",
    "\n",
    "print(\"BERT tokenizer with custom vocabulary: \", tokenizer_custom.text_to_tokens(SAMPLES))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.3 Exercise: Train a Larger Vocabulary \n",
    "\n",
    "Correct the \"FIXME\" lines to train a BERT tokenizer with a vocabulary size of 15,000. Check the [solution](solutions/ex3.2.3.ipynb) if you need to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The new vocabulary size  :  15000\n"
     ]
    }
   ],
   "source": [
    "# Train a larger vocabulary \n",
    "vocab_size= 15000\n",
    "my_bert_tokenizer_15k= BertWordPieceTokenizer()\n",
    "my_bert_tokenizer_15k.train(files=text_corpus, vocab_size=vocab_size, min_frequency=1,\n",
    "                            special_tokens=special_tokens, show_progress=True, wordpieces_prefix=\"##\")\n",
    "\n",
    "print(\" The new vocabulary size  : \", len(my_bert_tokenizer_15k.get_vocab()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3.3 Launch BERT Pretraining with NeMo\n",
    "\n",
    "We will use the model configuration for on-the-fly data preprocessing, [bert_pretraining_from_text_config.yaml](nemo/examples/nlp/language_modeling/conf/bert_pretraining_from_text_config.yaml), along with a training script, [bert_pretraining.py](nemo/examples/nlp/language_modeling/bert_pretraining.py). The YAML configuration file provides the parameters needed by the training script, and the parameter values can be overridden as needed. \n",
    "\n",
    "You'll learn more about NeMo configuration files and scripts in a later module.  For now, we'll just note a few important YAML keys in the configuration file:\n",
    "- `trainer`: Training process parameters such as the number of GPUs, Mixed precision training, number of epochs, etc.\n",
    "- `model.only_mlm_loss`: Use masked language model without next sentence prediction\n",
    "- `model.mask_prob`: Probability of masking a token in the input text during data processing\n",
    "- `model.train_ds`/`model.validation_ds`: datasets parameters\n",
    "- `model.tokenizer`: tokenizer parameters\n",
    "- `model.language_model`: language model architecture parameters\n",
    "- `model.optim`: Optimizer parameters\n",
    "\n",
    "Find more details about bert_pretraining parameters in the [NeMo documentation](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/nlp/bert_pretraining.html#quick-start-guide).\n",
    "\n",
    "For BERT offline pretraining with preprocessed data, use the dedicated configuration, [bert_pretraining_from_preprocessed_config.yaml](nemo/examples/nlp/language_modeling/conf/bert_pretraining_from_preprocessed_config.yaml)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# BERT Pretraining from Text\n",
      "name: &name PretrainingBERTFromText\n",
      "trainer:\n",
      "  gpus: 1 # the number of gpus, 0 for CPU, or list with gpu indices\n",
      "  num_nodes: 1\n",
      "  max_epochs: 2 # the number of training epochs\n",
      "  max_steps: null # precedence over max_epochs\n",
      "  accumulate_grad_batches: 1 # accumulates grads every k batches\n",
      "  precision: 16 # 16 to use AMP\n",
      "  amp_level: O1 # O1 or O2 if using AMP\n",
      "  accelerator: ddp\n",
      "  gradient_clip_val: 0.0\n",
      "  log_every_n_steps: 1\n",
      "  val_check_interval: 1.0 # check once per epoch .25 for 4 times per epoch\n",
      "  checkpoint_callback: false # provided by exp_manager\n",
      "  logger: false # provided by exp_manager\n",
      "\n",
      "model:\n",
      "  nemo_path: null # exported .nemo path\n",
      "  only_mlm_loss: false # only use masked language model without next sentence prediction\n",
      "  num_tok_classification_layers: 1 # number of token classification head output layers\n",
      "  num_seq_classification_layers: 2 # number of sequence classification head output layers\n",
      "  max_seq_length: 128\n",
      "  # The maximum total input sequence length after tokenization. Sequences longer than this\n",
      "  # will be truncated, and sequences shorter than this will be padded.\n",
      "  mask_prob: 0.15\n",
      "  # Probability of masking a token in the input text during data processing.\n",
      "  short_seq_prob: 0.1\n",
      "  # Probability of having a sequence shorter than the maximum sequence length `max_seq_length` in data processing.\",\n",
      "\n",
      "  language_model:\n",
      "    pretrained_model_name: bert-base-uncased\n",
      "    lm_checkpoint: null\n",
      "    config:\n",
      "      attention_probs_dropout_prob: 0.1\n",
      "      hidden_act: gelu\n",
      "      hidden_dropout_prob: 0.1\n",
      "      hidden_size: 768\n",
      "      initializer_range: 0.02\n",
      "      intermediate_size: 3072\n",
      "      max_position_embeddings: 512\n",
      "      num_attention_heads: 12\n",
      "      num_hidden_layers: 12\n",
      "      type_vocab_size: 2\n",
      "      vocab_size: 30522\n",
      "    config_file: null # json file, precedence over config\n",
      "\n",
      "  tokenizer:\n",
      "    tokenizer_name: ${model.language_model.pretrained_model_name} # tokenizer that inherits from TokenizerSpec\n",
      "    vocab_file: null # path to vocab file\n",
      "    tokenizer_model: null # tokenizer model for sentencepiece\n",
      "    special_tokens: # only necessary for adding transformer/bert-specific special tokens to tokenizer if the tokenizer does not already have these inherently.\n",
      "        unk_token: '[UNK]'\n",
      "        sep_token: '[SEP]'\n",
      "        pad_token: '[PAD]'\n",
      "        bos_token: '[CLS]'\n",
      "        mask_token: '[MASK]'\n",
      "        eos_token: '[SEP]'\n",
      "        cls_token: '[CLS]'\n",
      "\n",
      "  train_ds:\n",
      "    data_file: null # path to data file\n",
      "    max_seq_length: ${model.max_seq_length}\n",
      "    mask_prob: ${model.mask_prob}\n",
      "    short_seq_prob: ${model.short_seq_prob}\n",
      "    batch_size: 16 # per GPU\n",
      "    shuffle: true\n",
      "    num_samples: -1\n",
      "    num_workers: 2\n",
      "    drop_last: false\n",
      "    pin_memory: false\n",
      "\n",
      "  validation_ds:\n",
      "    data_file: null # path to data file\n",
      "    max_seq_length: ${model.max_seq_length}\n",
      "    mask_prob: ${model.mask_prob}\n",
      "    short_seq_prob: ${model.short_seq_prob}\n",
      "    batch_size: 16 # per GPU\n",
      "    shuffle: false\n",
      "    num_samples: -1\n",
      "    num_workers: 2\n",
      "    drop_last: false\n",
      "    pin_memory: false\n",
      "\n",
      "  optim:\n",
      "    name: adamw\n",
      "    lr: 3e-5\n",
      "    weight_decay: 0.0\n",
      "\n",
      "    sched:\n",
      "      name: CosineAnnealing\n",
      "      warmup_steps: null\n",
      "      warmup_ratio: 0.1\n",
      "      min_lr: 0.0\n",
      "      last_epoch: -1\n",
      "\n",
      "\n",
      "exp_manager:\n",
      "  exp_dir: null # where to store logs and checkpoints\n",
      "  name: *name # name of experiment\n",
      "  create_tensorboard_logger: True\n",
      "  create_checkpoint_callback: True\n",
      "\n",
      "hydra:\n",
      "  run:\n",
      "    dir: .\n",
      "  job_logging:\n",
      "    root:\n",
      "      handlers: null"
     ]
    }
   ],
   "source": [
    "# Show the configuration file\n",
    "! cat nemo/examples/nlp/language_modeling/conf/bert_pretraining_from_text_config.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo W 2022-07-28 08:37:28 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/omegaconf/basecontainer.py:225: UserWarning: cfg.pretty() is deprecated and will be removed in a future version.\n",
      "    Use OmegaConf.to_yaml(cfg)\n",
      "    \n",
      "      warnings.warn(\n",
      "    \n",
      "[NeMo I 2022-07-28 08:37:28 bert_pretraining:28] Config:\n",
      "     name: PretrainingBERTFromText\n",
      "    trainer:\n",
      "      gpus: 1\n",
      "      num_nodes: 1\n",
      "      max_epochs: 2\n",
      "      max_steps: null\n",
      "      accumulate_grad_batches: 1\n",
      "      precision: 16\n",
      "      amp_level: O1\n",
      "      accelerator: ddp\n",
      "      gradient_clip_val: 0.0\n",
      "      log_every_n_steps: 1\n",
      "      val_check_interval: 1.0\n",
      "      checkpoint_callback: false\n",
      "      logger: false\n",
      "    model:\n",
      "      nemo_path: null\n",
      "      only_mlm_loss: false\n",
      "      num_tok_classification_layers: 1\n",
      "      num_seq_classification_layers: 2\n",
      "      max_seq_length: 128\n",
      "      mask_prob: 0.15\n",
      "      short_seq_prob: 0.1\n",
      "      language_model:\n",
      "        pretrained_model_name: bert-base-uncased\n",
      "        lm_checkpoint: null\n",
      "        config:\n",
      "          attention_probs_dropout_prob: 0.1\n",
      "          hidden_act: gelu\n",
      "          hidden_dropout_prob: 0.1\n",
      "          hidden_size: 768\n",
      "          initializer_range: 0.02\n",
      "          intermediate_size: 3072\n",
      "          max_position_embeddings: 512\n",
      "          num_attention_heads: 12\n",
      "          num_hidden_layers: 12\n",
      "          type_vocab_size: 2\n",
      "          vocab_size: 30522\n",
      "        config_file: null\n",
      "      tokenizer:\n",
      "        tokenizer_name: ${model.language_model.pretrained_model_name}\n",
      "        vocab_file: /dli/task/data/vocab.txt\n",
      "        tokenizer_model: null\n",
      "        special_tokens:\n",
      "          unk_token: '[UNK]'\n",
      "          sep_token: '[SEP]'\n",
      "          pad_token: '[PAD]'\n",
      "          bos_token: '[CLS]'\n",
      "          mask_token: '[MASK]'\n",
      "          eos_token: '[SEP]'\n",
      "          cls_token: '[CLS]'\n",
      "      train_ds:\n",
      "        data_file: /dli/task/data/train.txt\n",
      "        max_seq_length: ${model.max_seq_length}\n",
      "        mask_prob: ${model.mask_prob}\n",
      "        short_seq_prob: ${model.short_seq_prob}\n",
      "        batch_size: 16\n",
      "        shuffle: true\n",
      "        num_samples: -1\n",
      "        num_workers: 2\n",
      "        drop_last: false\n",
      "        pin_memory: false\n",
      "      validation_ds:\n",
      "        data_file: /dli/task/data/test.txt\n",
      "        max_seq_length: ${model.max_seq_length}\n",
      "        mask_prob: ${model.mask_prob}\n",
      "        short_seq_prob: ${model.short_seq_prob}\n",
      "        batch_size: 16\n",
      "        shuffle: false\n",
      "        num_samples: -1\n",
      "        num_workers: 2\n",
      "        drop_last: false\n",
      "        pin_memory: false\n",
      "      optim:\n",
      "        name: adamw\n",
      "        lr: 3.0e-05\n",
      "        weight_decay: 0.0\n",
      "        sched:\n",
      "          name: CosineAnnealing\n",
      "          warmup_steps: null\n",
      "          warmup_ratio: 0.1\n",
      "          min_lr: 0.0\n",
      "          last_epoch: -1\n",
      "    exp_manager:\n",
      "      exp_dir: null\n",
      "      name: PretrainingBERTFromText\n",
      "      create_tensorboard_logger: true\n",
      "      create_checkpoint_callback: true\n",
      "    \n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "Using native 16bit precision.\n",
      "[NeMo I 2022-07-28 08:37:28 exp_manager:216] Experiments will be logged at /dli/task/nemo_experiments/PretrainingBERTFromText/2022-07-28_08-37-28\n",
      "[NeMo I 2022-07-28 08:37:28 exp_manager:563] TensorboardLogger has been set up\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[NeMo W 2022-07-28 08:37:28 auto_tokenizer:169] ['[UNK]'] \n",
      "     will be added to the vocabulary.\n",
      "    Please resize your model accordingly, see NLP_Tokenizers.ipynb for more details.\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 96.73it/s]\n",
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 3223.91it/s]\n",
      "[NeMo W 2022-07-28 08:37:28 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/nemo/core/classes/modelPT.py:243: UserWarning: update_node() is deprecated, use OmegaConf.update(). (Since 2.0)\n",
      "      self.cfg.update_node(config_path, return_path)\n",
      "    \n",
      "Downloading: 100%|███████████████████████████| 440M/440M [00:07<00:00, 59.5MB/s]\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[NeMo I 2022-07-28 08:37:39 modelPT:748] Optimizer config = AdamW (\n",
      "    Parameter Group 0\n",
      "        amsgrad: False\n",
      "        betas: (0.9, 0.999)\n",
      "        eps: 1e-08\n",
      "        lr: 3e-05\n",
      "        weight_decay: 0.0\n",
      "    )\n",
      "[NeMo I 2022-07-28 08:37:39 lr_scheduler:617] Scheduler \"<nemo.core.optim.lr_scheduler.CosineAnnealing object at 0x7f9b8da50490>\" \n",
      "    will be used during training (effective maximum steps = 86) - \n",
      "    Parameters : \n",
      "    (warmup_steps: null\n",
      "    warmup_ratio: 0.1\n",
      "    min_lr: 0.0\n",
      "    last_epoch: -1\n",
      "    max_steps: 86\n",
      "    )\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "[NeMo I 2022-07-28 08:37:39 modelPT:748] Optimizer config = AdamW (\n",
      "    Parameter Group 0\n",
      "        amsgrad: False\n",
      "        betas: (0.9, 0.999)\n",
      "        eps: 1e-08\n",
      "        lr: 3e-05\n",
      "        weight_decay: 0.0\n",
      "    )\n",
      "[NeMo I 2022-07-28 08:37:39 lr_scheduler:617] Scheduler \"<nemo.core.optim.lr_scheduler.CosineAnnealing object at 0x7f9b8d9a8040>\" \n",
      "    will be used during training (effective maximum steps = 86) - \n",
      "    Parameters : \n",
      "    (warmup_steps: null\n",
      "    warmup_ratio: 0.1\n",
      "    min_lr: 0.0\n",
      "    last_epoch: -1\n",
      "    max_steps: 86\n",
      "    )\n",
      "initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "INFO:root:Added key: store_based_barrier_key:1 to store for rank: 0\n",
      "\n",
      "  | Name                  | Type                           | Params\n",
      "-------------------------------------------------------------------------\n",
      "0 | bert_model            | BertEncoder                    | 109 M \n",
      "1 | mlm_classifier        | BertPretrainingTokenClassifier | 24.1 M\n",
      "2 | mlm_loss              | SmoothedCrossEntropyLoss       | 0     \n",
      "3 | nsp_classifier        | SequenceClassifier             | 592 K \n",
      "4 | nsp_loss              | CrossEntropyLoss               | 0     \n",
      "5 | agg_loss              | AggregatorLoss                 | 0     \n",
      "6 | validation_perplexity | Perplexity                     | 0     \n",
      "-------------------------------------------------------------------------\n",
      "110 M     Trainable params\n",
      "0         Non-trainable params\n",
      "110 M     Total params\n",
      "442.788   Total estimated model params size (MB)\n",
      "[NeMo W 2022-07-28 08:37:42 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "      warnings.warn(*args, **kwargs)\n",
      "    \n",
      "Validation sanity check: 100%|████████████████████| 1/1 [00:00<00:00,  1.39it/s][NeMo I 2022-07-28 08:37:42 bert_lm_model:185] evaluation perplexity 26169.925\n",
      "[NeMo W 2022-07-28 08:37:42 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "      warnings.warn(*args, **kwargs)\n",
      "    \n",
      "Epoch 0: 100%|█| 44/44 [00:04<00:00,  9.43it/s, loss=9.46, v_num=7-28, lr=1.86e-\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validating: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  4.45it/s]\u001b[A[NeMo I 2022-07-28 08:37:47 bert_lm_model:185] evaluation perplexity 23021.9546875\n",
      "Epoch 0: 100%|█| 44/44 [00:04<00:00,  8.93it/s, loss=9.46, v_num=7-28, lr=1.8e-5\n",
      "                                                                                \u001b[AEpoch 0, global step 42: val_loss reached 9.47038 (best 9.47038), saving model to \"/dli/task/nemo_experiments/PretrainingBERTFromText/2022-07-28_08-37-28/checkpoints/PretrainingBERTFromText--val_loss=9.47-epoch=0.ckpt\" as top 3\n",
      "Epoch 1: 100%|█| 44/44 [00:04<00:00,  9.36it/s, loss=9.04, v_num=7-28, lr=4.86e-\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validating: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  4.68it/s]\u001b[A[NeMo I 2022-07-28 08:37:58 bert_lm_model:185] evaluation perplexity 20978.91875\n",
      "Epoch 1: 100%|█| 44/44 [00:04<00:00,  8.90it/s, loss=9.04, v_num=7-28, lr=1.22e-\n",
      "                                                                                \u001b[AEpoch 1, global step 85: val_loss reached 9.36776 (best 9.36776), saving model to \"/dli/task/nemo_experiments/PretrainingBERTFromText/2022-07-28_08-37-28/checkpoints/PretrainingBERTFromText--val_loss=9.37-epoch=1.ckpt\" as top 3\n",
      "Saving latest checkpoint...\n",
      "Epoch 1: 100%|█| 44/44 [00:11<00:00,  3.98it/s, loss=9.04, v_num=7-28, lr=1.22e-\n",
      "[NeMo W 2022-07-28 08:38:04 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/nemo/core/classes/modelPT.py:308: UserWarning: update_node() is deprecated, use OmegaConf.update(). (Since 2.0)\n",
      "      conf.update_node(conf_path, item.path)\n",
      "    \n",
      "CPU times: user 1.04 s, sys: 380 ms, total: 1.42 s\n",
      "Wall time: 1min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Override the parameters specific to our data; run only two epochs for now\n",
    "! python nemo/examples/nlp/language_modeling/bert_pretraining.py \\\n",
    "    model.train_ds.data_file=/dli/task/data/train.txt\\\n",
    "    model.validation_ds.data_file=/dli/task/data/test.txt\\\n",
    "    model.tokenizer.vocab_file=/dli/task/data/vocab.txt\\\n",
    "    model.train_ds.batch_size=16 \\\n",
    "    trainer.max_epochs=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3.1 TensorBoard Visualization\n",
    "Open [TensorBoard](/tensorboard/) in your browser.  Then, click the link to see graphs of experiment metrics like loss and accuracy saved in the `nemo_experiments` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3.2 Practical Considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretraining a Transformer-based language models does not require labeled text corpus datasets. However, it does require a large amount of data and compute time.  For example, pretraining a BERT model on the [English Wikipedia](https://huggingface.co/datasets/wikipedia) + [bookcorpus](https://huggingface.co/datasets/bookcorpus) using an NVIDIA DGX-1 server with 8 V100 GPUs takes about 6 days in mixed precision mode. You can find out more about BERT training and fine-tuning performance at https://catalog.ngc.nvidia.com/orgs/nvidia/resources/bert_for_pytorch/performance.\n",
    "\n",
    "On the other hand, fine-tuning a Transformer-based model is less computationally intensive, but requires labeled data. The lab in Part 2 will focus on fine-tuning BERT models for downstream NLP tasks such as text classification and named entity recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<h2 style=\"color:green;\">Congratulations!</h2>\n",
    "\n",
    "You've completed the BERT pretraining notebook!  \n",
    "\n",
    "You've learned:\n",
    "* How to train a BERT tokenizer\n",
    "* How to pretrain a BERT language model with NeMo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "186px",
    "left": "619px",
    "top": "238px",
    "width": "213px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
