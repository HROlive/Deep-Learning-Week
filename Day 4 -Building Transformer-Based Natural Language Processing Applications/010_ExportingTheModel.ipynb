{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0 Exporting the Model\n",
    "In this notebook, you'll explore options for exporting a BERT checkpoint trained using PyTorch, to NVIDIA Triton Inference Server.\n",
    "\n",
    "**[1.1 Overview: Optimization and Performance](#1.1-Overview:-Optimization-and-Performance)<br>**\n",
    "**[1.2 Export a BERT Checkpoint](#1.2-Export-a-BERT-Checkpoint)<br>**\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; [1.2.1 Triton Model Repository](#1.2.1-Triton-Model-Repository)<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; [1.2.2 TorchScript Export](#1.2.2-TorchScript-Export)<br>\n",
    "**[1.3 Test Our Export](#1.3-Test-Our-Export)<br>**\n",
    "**[1.4 Beyond TorchScript](#1.4-Beyond-TorchScript)<br>**\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; [1.4.1 Exercise: Enable TensorRT Optimization](#1.4.1-Exercise:-Enable-TensorRT-Optimization)<br>\n",
    "**[1.5 Performance Comparison](#1.5-Performance-Comparison)<br>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Overview: Optimization and Performance\n",
    "Optimization of the trained model will have a fairly dramatic impact on the inference performance, measured in bandwidth and latency. Even if the project requirements do not justify investing engineering effort into advanced techniques, such as knowledge distillation or pruning, a fair amount of model performance improvement can be achieved by using model optimization tools. The diagram below illustrates the difference in inference performance between a model deployed using non-optimized TensorFlow, the same model post-processed with TensorRT, and a model fully optimized with TensorRT. \n",
    "\n",
    "<img src=\"images/TFvTRT.jpg\" alt=\"Header\" style=\"width: 600px;\"/>\n",
    "\n",
    "Modern inference servers typically support substantially more than one model format to cater to a wider range of projects, tools, and preferences. Since in this class we are working with a BERT checkpoint trained using PyTorch, and we are deploying it with Triton Inference Server, we will focus on options for deploying PyTorch-based models. These include:\n",
    "   - PyTorch JIT / TorchScript\n",
    "   - ONNX runtime\n",
    "   - ONNX-TensorRT\n",
    "   - TensorRT\n",
    "    \n",
    "It's important to point out that Triton Server supports a much broader set of deployment mechanisms including:\n",
    "   - TensorFlow GraphDef\n",
    "   - TensorFlow saved model\n",
    "   - Caffe 2 exports\n",
    "   - Custom models (which can be any custom executable)\n",
    "\n",
    "In this section we will look at how to deploy a model using some of the deployment engines listed above and the impact each has on performance. We will also experiment with some of the key settings, namely the batch size and numerical precision (FP32 and FP16)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 Export a BERT Checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BERT model checkpoint we want to deploy, <code>bert_qa.pt</code>, should be located in your `data` directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/bert_qa.pt\n"
     ]
    }
   ],
   "source": [
    "!ls data/*.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file is a standard checkpoint of a BERT-Large network, fine-tuned on the [Stanford Question Answering Dataset (SQuAD)](https://arxiv.org/abs/1606.05250). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Scripts\n",
    "As we explore various deployment configurations, we'll repeat some steps over and over.  Therefore, we'll use some helper scripts to partially automate the process so that we can focus our attention on the configuration settings and results.  You can explore the code details yourself if you are curious:\n",
    "\n",
    "- [utilities/wait_for_triton_server.sh](utilities/wait_for_triton_server.sh): Check the \"live\" and \"ready\" status of the Triton server via the API\n",
    "- [deployer/deployer.py](deployer/deployer.py): Convert a checkpoint to a deployable model and export it\n",
    "- [uitlities/run_perf_client_local.sh](utilities/run_perf_client_local.sh): Measure performance with the [perf_client](https://docs.nvidia.com/deeplearning/triton-inference-server/master-user-guide/docs/perf_client.html) application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Triton server has been deployed in a container and is available to us at host \"triton\" on port \"8000\". Run the next cell to to check for a \"200 OK\" HTTP response from the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for Triton Server to be ready at triton:8000...\n",
      "200\n",
      "Triton Server is ready!\n"
     ]
    }
   ],
   "source": [
    "# Set the server hostname and check it - you should get a message that \"Triton Server is ready!\"\n",
    "tritonServerHostName = \"triton\"\n",
    "!./utilities/wait_for_triton_server.sh {tritonServerHostName}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.1 Triton Model Repository\n",
    "When Triton Server is started, it is typically configured to observe a local or remote file system where models are hosted. The directory which is being observed is called a *model repository*. A typical command to start the Triton Server identifies the location of the model repository with an option:<br>\n",
    "```bash\n",
    "tritonserver --model-repository=\"/path/to/model/repository\"\n",
    "```\n",
    "\n",
    "The model repository needs to have the following layout:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "<model-repository-path>/\n",
    "  <model-name>/\n",
    "    [config.pbtxt]\n",
    "    [<output-labels-file> ...]\n",
    "    <version>/\n",
    "      <model-definition-file>\n",
    "    <version>/\n",
    "      <model-definition-file>\n",
    "    ...\n",
    "  <model-name>/\n",
    "    [config.pbtxt]\n",
    "    [<output-labels-file> ...]\n",
    "    <version>/\n",
    "      <model-definition-file>\n",
    "    <version>/\n",
    "      <model-definition-file>\n",
    "    ...\n",
    "  ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lab container is configured to use the <code>./model_repository</code> folder as the model repository, so any change within this folder will affect the behavior of Triton Server.<br/>\n",
    "\n",
    "In order to expose a new model to Triton you need to: <br/>\n",
    "   1. Create a new model folder in the model repository. The name of the folder needs to reflect the name of the service you will be exposing to your users/applications.<br/>\n",
    "   2. Within the model folder, create a <code>config.pbtxt</code> file that contains the basic serving configuration for the model<br/>\n",
    "   3. Also within the model folder, create at least one folder containing a copy of the model. The name of the folder reflects the version name of the model. You can create and host multiple versions of the same model.<br/>\n",
    "    \n",
    "Next, we'll walk through the process of exporting the model to Triton."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.2 TorchScript Export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the lab we will:\n",
    "   - Convert the PyTorch checkpoint into [TorchScript](https://pytorch.org/docs/stable/jit.html#torchscript)\n",
    "   - Generate the Triton configuration file\n",
    "   - Deploy the created assets to our model repository\n",
    "Please execute the cells below. Since we are loading a PyTorch checkpoint and converting it into TorchScript, it might take a minute or two to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelName = \"bertQA-torchscript\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deploying model bertQA-torchscript in format pytorch_libtorch\n",
      "/opt/conda/lib/python3.6/site-packages/torch/jit/_recursive.py:160: UserWarning: 'bias' was found in ScriptModule constants,  but it is a non-constant parameter. Consider removing it.\n",
      "  \" but it is a non-constant {}. Consider removing it.\".format(name, hint))\n",
      "\n",
      "conversion correctness test results\n",
      "-----------------------------------\n",
      "maximal absolute error over dataset (L_inf):  8.344650268554688e-06\n",
      "\n",
      "average L_inf error over output tensors:  8.106231689453125e-06\n",
      "variance of L_inf error over output tensors:  7.579122514774402e-14\n",
      "stddev of L_inf error over output tensors:  2.7530206164819037e-07\n",
      "\n",
      "time of error check of native model:  0.7872045040130615 seconds\n",
      "time of error check of ts model:  1.910688877105713 seconds\n",
      "\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "!python ./deployer/deployer.py \\\n",
    "    --ts-script \\\n",
    "    --save-dir ./candidatemodels \\\n",
    "    --triton-model-name {modelName} \\\n",
    "    --triton-model-version 1 \\\n",
    "    --triton-max-batch-size 8 \\\n",
    "    --triton-dyn-batching-delay 0 \\\n",
    "    --triton-engine-count 1 \\\n",
    "    -- --checkpoint \"/dli/task/data/bert_qa.pt\" \\\n",
    "    --config_file ./bert_config.json \\\n",
    "    --vocab_file ./vocab \\\n",
    "    --predict_file ./squad/v1.1/dev-v1.1.json \\\n",
    "    --do_lower_case \\\n",
    "    --batch_size=8 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `deployer.py` script loads the `bert_qa.pt` checkpoint, deploys it in `ts-script` format into a folder called `bertQA-torchscript`, and marks it as version `1`. We will discuss some of the more advanced settings later. For now, let's inspect the files generated by the script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 16\n",
      "drwxr-xr-x 3 root root 4096 Jul 28 12:41 .\n",
      "drwxr-xr-x 3 root root 4096 Jul 28 12:41 ..\n",
      "drwxr-xr-x 2 root root 4096 Jul 28 12:41 1\n",
      "-rw-r--r-- 1 root root  568 Jul 28 12:41 config.pbtxt\n",
      "total 1309292\n",
      "drwxr-xr-x 2 root root       4096 Jul 28 12:41 .\n",
      "drwxr-xr-x 3 root root       4096 Jul 28 12:41 ..\n",
      "-rw-r--r-- 1 root root 1340706048 Jul 28 12:41 model.pt\n"
     ]
    }
   ],
   "source": [
    "!ls -al ./candidatemodels/bertQA-torchscript/\n",
    "!ls -al ./candidatemodels/bertQA-torchscript/1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the script exported the model into the TorchScript format and saved it as `model.pt`. It also generated the `config.pbtxt` file. <br> \n",
    "Let's take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: \"bertQA-torchscript\"\n",
      "platform: \"pytorch_libtorch\"\n",
      "max_batch_size: 8\n",
      "input [\n",
      "{\n",
      "    name: \"input__0\"\n",
      "    data_type: TYPE_INT64\n",
      "    dims: [384]\n",
      "},\n",
      "{\n",
      "    name: \"input__1\"\n",
      "    data_type: TYPE_INT64\n",
      "    dims: [384]\n",
      "},\n",
      "{\n",
      "    name: \"input__2\"\n",
      "    data_type: TYPE_INT64\n",
      "    dims: [384]\n",
      "}\n",
      "]\n",
      "output [\n",
      "{\n",
      "    name: \"output__0\"\n",
      "    data_type: TYPE_FP32\n",
      "    dims: [384]\n",
      "}, \n",
      "{\n",
      "    name: \"output__1\"\n",
      "    data_type: TYPE_FP32\n",
      "    dims: [384]\n",
      "}\n",
      "]\n",
      "optimization {\n",
      "  cuda {\n",
      "    graphs: 0\n",
      "  }\n",
      "}\n",
      "instance_group [\n",
      "    {\n",
      "        count: 1\n",
      "        kind: KIND_GPU\n",
      "        gpus: [ 0 ]\n",
      "    }\n",
      "]"
     ]
    }
   ],
   "source": [
    "!cat ./candidatemodels/bertQA-torchscript/config.pbtxt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The configuration file is fairly simple and defines:\n",
    "   - Name of the model\n",
    "   - Type of platform to be used for inference; in this case `pytorch_libtorch`\n",
    "   - Input and output dimensions used by the network\n",
    "   - Optimizations used; in this case GPU and the default TorchScript optimization \n",
    "   - Instance group configuration; in this case instance group count is set to one, meaning that only one copy of the model will be held in GPU memory (GPU 0 is being used).\n",
    "    \n",
    "To deploy the model, move the folder to the Triton model repository:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv ./candidatemodels/bertQA-torchscript model_repository/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations!  You have successfully deployed your first model to Triton Inference Server!\n",
    "\n",
    "We'll come back to discuss the detailed configuration later, but for now let's see how our model is performing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  1.3 Test Our Export\n",
    "Execute the cells below to start an inference process and make a simple measurement of inference performance. First, we'll set up some configuration. `maxConcurrency` is set to two, meaning that the stress test will be executed twice. The first run will use just a single thread and the second one will use two threads to query the server. Without turning on the concurrent model execution or dynamic batching features, what do you think will be the impact on performance of running two processes querying the server? Do you think:<br/>\n",
    "- Bandwidth will increase or decrease?<br/>\n",
    "- Latency will increase or decrease?<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "modelVersion=\"1\"\n",
    "precision=\"fp32\"\n",
    "batchSize=\"1\"\n",
    "maxLatency=\"500\"\n",
    "maxClientThreads=\"10\"\n",
    "maxConcurrency=\"2\"\n",
    "dockerBridge=\"host\"\n",
    "resultsFolderName=\"1\"\n",
    "profilingData=\"utilities/profiling_data_int64\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for Triton Server to be ready at triton:8000...\n",
      "200\n",
      "Triton Server is ready!\n",
      "WARNING: Overriding max_threads specification to ensure requested concurrency range.\n",
      "*** Measurement Settings ***\n",
      "  Batch size: 1\n",
      "  Measurement window: 3000 msec\n",
      "  Latency limit: 500 msec\n",
      "  Concurrency limit: 2 concurrent requests\n",
      "  Using synchronous calls for inference\n",
      "  Stabilizing using average latency\n",
      "\n",
      "Request concurrency: 1\n",
      "  Pass [1] throughput: 25 infer/sec. Avg latency: 43622 usec (std 80719 usec)\n",
      "  Pass [2] throughput: 29.3333 infer/sec. Avg latency: 34161 usec (std 53 usec)\n",
      "  Pass [3] throughput: 29 infer/sec. Avg latency: 34165 usec (std 60 usec)\n",
      "  Pass [4] throughput: 29.3333 infer/sec. Avg latency: 34150 usec (std 40 usec)\n",
      "  Client: \n",
      "    Request count: 88\n",
      "    Throughput: 29.3333 infer/sec\n",
      "    Avg latency: 34150 usec (standard deviation 40 usec)\n",
      "    p50 latency: 34147 usec\n",
      "    p90 latency: 34198 usec\n",
      "    p95 latency: 34218 usec\n",
      "    p99 latency: 34244 usec\n",
      "    Avg HTTP time: 34145 usec (send 4 usec + response wait 34140 usec + receive 1 usec)\n",
      "  Server: \n",
      "    Inference count: 105\n",
      "    Execution count: 105\n",
      "    Successful request count: 105\n",
      "    Avg request latency: 33824 usec (overhead 3 usec + queue 698 usec + compute input 56 usec + compute infer 31300 usec + compute output 1767 usec)\n",
      "\n",
      "Request concurrency: 2\n",
      "  Pass [1] throughput: 29 infer/sec. Avg latency: 68345 usec (std 68 usec)\n",
      "  Pass [2] throughput: 29 infer/sec. Avg latency: 68460 usec (std 224 usec)\n",
      "  Pass [3] throughput: 28.6667 infer/sec. Avg latency: 69109 usec (std 2322 usec)\n",
      "  Client: \n",
      "    Request count: 86\n",
      "    Throughput: 28.6667 infer/sec\n",
      "    Avg latency: 69109 usec (standard deviation 2322 usec)\n",
      "    p50 latency: 68507 usec\n",
      "    p90 latency: 69800 usec\n",
      "    p95 latency: 72442 usec\n",
      "    p99 latency: 77534 usec\n",
      "    Avg HTTP time: 69007 usec (send 8 usec + response wait 68997 usec + receive 2 usec)\n",
      "  Server: \n",
      "    Inference count: 104\n",
      "    Execution count: 104\n",
      "    Successful request count: 104\n",
      "    Avg request latency: 68587 usec (overhead 4 usec + queue 35176 usec + compute input 58 usec + compute infer 31714 usec + compute output 1635 usec)\n",
      "\n",
      "Inferences/Second vs. Client Average Batch Latency\n",
      "Concurrency: 1, throughput: 29.3333 infer/sec, latency 34150 usec\n",
      "Concurrency: 2, throughput: 28.6667 infer/sec, latency 69109 usec\n"
     ]
    }
   ],
   "source": [
    "!./utilities/run_perf_client_local.sh \\\n",
    "                    {modelName} \\\n",
    "                    {modelVersion} \\\n",
    "                    {precision} \\\n",
    "                    {batchSize} \\\n",
    "                    {maxLatency} \\\n",
    "                    {maxClientThreads} \\\n",
    "                    {maxConcurrency} \\\n",
    "                    {tritonServerHostName} \\\n",
    "                    {dockerBridge} \\\n",
    "                    {resultsFolderName} \\\n",
    "                    {profilingData}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything went okay you should have been presented with output similar to the following example result, showing the inference performance across two different configurations.<br/>\n",
    "<img src=\"images/InferenceJob1.png\" alt=\"Example output of inference job 1\" style=\"width: 1200px;\"/>\n",
    "\n",
    "If you happened to get \"error: failed to get model metatdata\", try running the cell again.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.4 Beyond TorchScript\n",
    "\n",
    "Let's investigate a different route for model deployment onto Triton, namely <a href=\"https://onnx.ai\">Open Neural Network Exchange (ONNX)</a>. ONNX is an open format for representation and exchange of neural network models. It defines a common set of operators that are used to build common models, as well as a file format for exchanging them. The advantage of ONNX is that it is relatively widely adopted and can be used to exchange models between <a href=\"https://onnx.ai/supported-tools.html\">a wide range of deep learning tools</a>, such as deep learning frameworks or deployment tools. This also includes TensorRT, which can consume ONNX models. </br>\n",
    "\n",
    "As before, start by exporting the model, but this time using the ONNX format. We will take advantage of the export tool that we used earlier, but change the export format from <code>ts-script</code> to <code>onnx</code>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelName = \"bertQA-onnx\"\n",
    "exportFormat = \"onnx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deploying model bertQA-onnx in format onnxruntime_onnx\n",
      "/opt/conda/lib/python3.6/site-packages/torch/onnx/utils.py:955: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input input__0\n",
      "  'Automatically generated names will be applied to each dynamic axes of input {}'.format(key))\n",
      "/opt/conda/lib/python3.6/site-packages/torch/onnx/utils.py:955: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input input__1\n",
      "  'Automatically generated names will be applied to each dynamic axes of input {}'.format(key))\n",
      "/opt/conda/lib/python3.6/site-packages/torch/onnx/utils.py:955: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input input__2\n",
      "  'Automatically generated names will be applied to each dynamic axes of input {}'.format(key))\n",
      "/opt/conda/lib/python3.6/site-packages/torch/onnx/utils.py:955: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input output__0\n",
      "  'Automatically generated names will be applied to each dynamic axes of input {}'.format(key))\n",
      "/opt/conda/lib/python3.6/site-packages/torch/onnx/utils.py:955: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input output__1\n",
      "  'Automatically generated names will be applied to each dynamic axes of input {}'.format(key))\n",
      "[libprotobuf WARNING google/protobuf/io/coded_stream.cc:604] Reading dangerously large protocol message.  If the message turns out to be larger than 2147483647 bytes, parsing will be halted for security reasons.  To increase the limit (or to disable these warnings), see CodedInputStream::SetTotalBytesLimit() in google/protobuf/io/coded_stream.h.\n",
      "[libprotobuf WARNING google/protobuf/io/coded_stream.cc:81] The total number of bytes read was 1336539973\n",
      "\n",
      "conversion correctness test results\n",
      "-----------------------------------\n",
      "maximal absolute error over dataset (L_inf):  0.00022935867309570312\n",
      "\n",
      "average L_inf error over output tensors:  0.0001423954963684082\n",
      "variance of L_inf error over output tensors:  6.657553323445124e-09\n",
      "stddev of L_inf error over output tensors:  8.159383140559784e-05\n",
      "\n",
      "time of error check of native model:  0.40985536575317383 seconds\n",
      "time of error check of onnx model:  18.74768328666687 seconds\n",
      "\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "!python ./deployer/deployer.py \\\n",
    "    --{exportFormat} \\\n",
    "    --save-dir ./candidatemodels \\\n",
    "    --triton-model-name {modelName} \\\n",
    "    --triton-model-version 1 \\\n",
    "    --triton-max-batch-size 8 \\\n",
    "    --triton-dyn-batching-delay 0 \\\n",
    "    --triton-engine-count 1 \\\n",
    "    -- --checkpoint ./data/bert_qa.pt \\\n",
    "    --config_file ./bert_config.json \\\n",
    "    --vocab_file ./vocab \\\n",
    "    --predict_file ./squad/v1.1/dev-v1.1.json \\\n",
    "    --do_lower_case \\\n",
    "    --batch_size=8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the <a href=\"https://github.com/pytorch/pytorch/blob/master/torch/csrc/jit/docs/serialization.md\">TorchScript serialization format</a>, the <a href=\"https://onnx.ai/get-started.html\">ONNX format</a> can be inspected quite easily (and parts are human readable). Lets have a look at the assets our export has generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 16\n",
      "drwxr-xr-x 3 root root 4096 Jul 28 12:43 .\n",
      "drwxr-xr-x 3 root root 4096 Jul 28 12:42 ..\n",
      "drwxr-xr-x 2 root root 4096 Jul 28 12:43 1\n",
      "-rw-r--r-- 1 root root  561 Jul 28 12:43 config.pbtxt\n",
      "total 1305228\n",
      "drwxr-xr-x 2 root root       4096 Jul 28 12:43 .\n",
      "drwxr-xr-x 3 root root       4096 Jul 28 12:43 ..\n",
      "-rw-r--r-- 1 root root 1336539973 Jul 28 12:43 model.onnx\n"
     ]
    }
   ],
   "source": [
    "!ls -al ./candidatemodels/bertQA-onnx/\n",
    "!ls -al ./candidatemodels/bertQA-onnx/1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we have a configuration file as well as a model, this time stored in ONNX format. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a couple of options for executing the ONNX-based export in Triton:\n",
    "- We can take advantage of ONNX runtime </br>\n",
    "- We can ask TensorRT to parse the ONNX assets in order to generate a TensorRT engine to use instead </br>\n",
    "\n",
    "We'll try both approaches and look at the impact this has on inference performance. In order to deploy the current ONNX model, move it to the model repository..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv ./candidatemodels/bertQA-onnx model_repository/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and run our stress testing code across 10 different levels of concurrency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: bertQA-onnx\n",
      "Waiting for Triton Server to be ready at triton:8000...\n",
      "200\n",
      "Triton Server is ready!\n",
      "WARNING: Overriding max_threads specification to ensure requested concurrency range.\n",
      "*** Measurement Settings ***\n",
      "  Batch size: 8\n",
      "  Measurement window: 3000 msec\n",
      "  Latency limit: 500 msec\n",
      "  Concurrency limit: 10 concurrent requests\n",
      "  Using synchronous calls for inference\n",
      "  Stabilizing using average latency\n",
      "\n",
      "Request concurrency: 1\n",
      "  Pass [1] throughput: 48 infer/sec. Avg latency: 166599 usec (std 818 usec)\n",
      "  Pass [2] throughput: 45.3333 infer/sec. Avg latency: 167320 usec (std 708 usec)\n",
      "  Pass [3] throughput: 48 infer/sec. Avg latency: 167514 usec (std 217 usec)\n",
      "  Client: \n",
      "    Request count: 18\n",
      "    Throughput: 48 infer/sec\n",
      "    Avg latency: 167514 usec (standard deviation 217 usec)\n",
      "    p50 latency: 167566 usec\n",
      "    p90 latency: 167692 usec\n",
      "    p95 latency: 167698 usec\n",
      "    p99 latency: 167901 usec\n",
      "    Avg HTTP time: 167461 usec (send 9 usec + response wait 167451 usec + receive 1 usec)\n",
      "  Server: \n",
      "    Inference count: 168\n",
      "    Execution count: 21\n",
      "    Successful request count: 21\n",
      "    Avg request latency: 167071 usec (overhead 4 usec + queue 14 usec + compute input 14 usec + compute infer 167025 usec + compute output 14 usec)\n",
      "\n",
      "Request concurrency: 2\n",
      "  Pass [1] throughput: 45.3333 infer/sec. Avg latency: 334876 usec (std 818 usec)\n",
      "  Pass [2] throughput: 45.3333 infer/sec. Avg latency: 334836 usec (std 984 usec)\n",
      "  Pass [3] throughput: 48 infer/sec. Avg latency: 335707 usec (std 877 usec)\n",
      "  Client: \n",
      "    Request count: 18\n",
      "    Throughput: 48 infer/sec\n",
      "    Avg latency: 335707 usec (standard deviation 877 usec)\n",
      "    p50 latency: 335389 usec\n",
      "    p90 latency: 336915 usec\n",
      "    p95 latency: 337079 usec\n",
      "    p99 latency: 337437 usec\n",
      "    Avg HTTP time: 335568 usec (send 21 usec + response wait 335545 usec + receive 2 usec)\n",
      "  Server: \n",
      "    Inference count: 176\n",
      "    Execution count: 22\n",
      "    Successful request count: 22\n",
      "    Avg request latency: 334974 usec (overhead 3 usec + queue 167304 usec + compute input 26 usec + compute infer 167618 usec + compute output 23 usec)\n",
      "\n",
      "Request concurrency: 3\n",
      "  Pass [1] throughput: 45.3333 infer/sec. Avg latency: 491891 usec (std 39376 usec)\n",
      "  Pass [2] throughput: 45.3333 infer/sec. Avg latency: 501886 usec (std 2291 usec)\n",
      "  Pass [3] throughput: 48 infer/sec. Avg latency: 502723 usec (std 2056 usec)\n",
      "  Client: \n",
      "    Request count: 18\n",
      "    Throughput: 48 infer/sec\n",
      "    Avg latency: 502723 usec (standard deviation 2056 usec)\n",
      "    p50 latency: 503073 usec\n",
      "    p90 latency: 504720 usec\n",
      "    p95 latency: 505872 usec\n",
      "    p99 latency: 505949 usec\n",
      "    Avg HTTP time: 502920 usec (send 12 usec + response wait 502906 usec + receive 2 usec)\n",
      "  Server: \n",
      "    Inference count: 168\n",
      "    Execution count: 21\n",
      "    Successful request count: 21\n",
      "    Avg request latency: 502495 usec (overhead 4 usec + queue 334940 usec + compute input 16 usec + compute infer 167521 usec + compute output 14 usec)\n",
      "\n",
      "Measured latency went over the set limit of 500 msec. \n",
      "Inferences/Second vs. Client Average Batch Latency\n",
      "Concurrency: 1, throughput: 48 infer/sec, latency 167514 usec\n",
      "Concurrency: 2, throughput: 48 infer/sec, latency 335707 usec\n",
      "Concurrency: 3, throughput: 48 infer/sec, latency 502723 usec\n"
     ]
    }
   ],
   "source": [
    "modelName = \"bertQA-onnx\"\n",
    "maxConcurrency = \"10\"\n",
    "batchSize = \"8\"\n",
    "print(\"Running: \"+modelName)\n",
    "!bash ./utilities/run_perf_client_local.sh \\\n",
    "                    {modelName} \\\n",
    "                    {modelVersion} \\\n",
    "                    {precision} \\\n",
    "                    {batchSize} \\\n",
    "                    {maxLatency} \\\n",
    "                    {maxClientThreads} \\\n",
    "                    {maxConcurrency} \\\n",
    "                    {tritonServerHostName} \\\n",
    "                    {dockerBridge} \\\n",
    "                    {resultsFolderName} \\\n",
    "                    {profilingData}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look at the results. Did we manage to run our benchmark at all 10 concurrency levels (or did the benchmark time out earlier)? What happened to the request latency in relation to the 500 ms time limit we configured?</br>\n",
    "\n",
    "Now let's export the ONNX model again, so that we can configure it for TensorRT execution.</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelName = \"bertQA-onnx-trt-fp16\"\n",
    "exportFormat = \"onnx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deploying model bertQA-onnx-trt-fp16 in format onnxruntime_onnx\n",
      "/opt/conda/lib/python3.6/site-packages/torch/onnx/utils.py:955: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input input__0\n",
      "  'Automatically generated names will be applied to each dynamic axes of input {}'.format(key))\n",
      "/opt/conda/lib/python3.6/site-packages/torch/onnx/utils.py:955: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input input__1\n",
      "  'Automatically generated names will be applied to each dynamic axes of input {}'.format(key))\n",
      "/opt/conda/lib/python3.6/site-packages/torch/onnx/utils.py:955: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input input__2\n",
      "  'Automatically generated names will be applied to each dynamic axes of input {}'.format(key))\n",
      "/opt/conda/lib/python3.6/site-packages/torch/onnx/utils.py:955: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input output__0\n",
      "  'Automatically generated names will be applied to each dynamic axes of input {}'.format(key))\n",
      "/opt/conda/lib/python3.6/site-packages/torch/onnx/utils.py:955: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input output__1\n",
      "  'Automatically generated names will be applied to each dynamic axes of input {}'.format(key))\n",
      "[libprotobuf WARNING google/protobuf/io/coded_stream.cc:604] Reading dangerously large protocol message.  If the message turns out to be larger than 2147483647 bytes, parsing will be halted for security reasons.  To increase the limit (or to disable these warnings), see CodedInputStream::SetTotalBytesLimit() in google/protobuf/io/coded_stream.h.\n",
      "[libprotobuf WARNING google/protobuf/io/coded_stream.cc:81] The total number of bytes read was 1336539973\n",
      "\n",
      "conversion correctness test results\n",
      "-----------------------------------\n",
      "maximal absolute error over dataset (L_inf):  0.00022935867309570312\n",
      "\n",
      "average L_inf error over output tensors:  0.0001423954963684082\n",
      "variance of L_inf error over output tensors:  6.657553323445124e-09\n",
      "stddev of L_inf error over output tensors:  8.159383140559784e-05\n",
      "\n",
      "time of error check of native model:  0.41291213035583496 seconds\n",
      "time of error check of onnx model:  16.886172771453857 seconds\n",
      "\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "!python ./deployer/deployer.py \\\n",
    "    --{exportFormat} \\\n",
    "    --save-dir ./candidatemodels \\\n",
    "    --triton-model-name {modelName} \\\n",
    "    --triton-model-version 1 \\\n",
    "    --triton-max-batch-size 8 \\\n",
    "    --triton-dyn-batching-delay 0 \\\n",
    "    --triton-engine-count 1 \\\n",
    "    -- --checkpoint ./data/bert_qa.pt \\\n",
    "    --config_file ./bert_config.json \\\n",
    "    --vocab_file ./vocab \\\n",
    "    --predict_file ./squad/v1.1/dev-v1.1.json \\\n",
    "    --do_lower_case \\\n",
    "    --batch_size=8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again the above command should have generated the ONNX export as well as a configuration file: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 16\n",
      "drwxr-xr-x 3 root root 4096 Jul 28 12:45 .\n",
      "drwxr-xr-x 3 root root 4096 Jul 28 12:44 ..\n",
      "drwxr-xr-x 2 root root 4096 Jul 28 12:44 1\n",
      "-rw-r--r-- 1 root root  570 Jul 28 12:45 config.pbtxt\n"
     ]
    }
   ],
   "source": [
    "!ls -al ./candidatemodels/bertQA-onnx-trt-fp16/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4.1 Exercise: Enable TensorRT Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to enable TensorRT, we need to add an additional section to the \"config.pbtxt\" configuration file. In particular, we need to add an additional segment to the <code>optimization</code> section:\n",
    "\n",
    "```text\n",
    "optimization {\n",
    "   execution_accelerators {\n",
    "      gpu_execution_accelerator : [ {\n",
    "         name : \"tensorrt\"\n",
    "         parameters { key: \"precision_mode\" value: \"FP16\" }\n",
    "      }]\n",
    "   }\n",
    "cuda { graphs: 0 }\n",
    "}\n",
    "```\n",
    "\n",
    "#### Exercise Steps:\n",
    "1. Modify [config.pbtxt](candidatemodels/bertQA-onnx-trt-fp16/config.pbtxt) to enable TensorRT. Feel free to look at the [solution](solutions/ex-1-4-1_config.pbtxt) as needed.\n",
    "2. Once you have saved your changes (Main menu: File -> Save File), move the folder to the model repository using the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config.pbxt code\n",
    "\n",
    "name: \"bertQA-onnx-trt-fp16\"\n",
    "platform: \"onnxruntime_onnx\"\n",
    "max_batch_size: 8\n",
    "input [\n",
    "{\n",
    "    name: \"input__0\"\n",
    "    data_type: TYPE_INT64\n",
    "    dims: [384]\n",
    "},\n",
    "{\n",
    "    name: \"input__1\"\n",
    "    data_type: TYPE_INT64\n",
    "    dims: [384]\n",
    "},\n",
    "{\n",
    "    name: \"input__2\"\n",
    "    data_type: TYPE_INT64\n",
    "    dims: [384]\n",
    "}\n",
    "]\n",
    "output [\n",
    "{\n",
    "    name: \"output__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [384]\n",
    "}, \n",
    "{\n",
    "    name: \"output__1\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [384]\n",
    "}\n",
    "]\n",
    "optimization {\n",
    "    execution_accelerators {\n",
    "      gpu_execution_accelerator : [ {\n",
    "         name : \"tensorrt\"\n",
    "         parameters { key: \"precision_mode\" value: \"FP16\" }\n",
    "      }]\n",
    "   }\n",
    "  cuda {\n",
    "    graphs: 0\n",
    "  }\n",
    "}\n",
    "instance_group [\n",
    "    {\n",
    "        count: 1\n",
    "        kind: KIND_GPU\n",
    "        gpus: [ 0 ]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv ./candidatemodels/bertQA-onnx-trt-fp16 model_repository/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Execute our profiling tool in the next cell and investigate the impact on performance. This could take a while to start, as we are waiting for the server to migrate the model to TensorRT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: bertQA-onnx-trt-fp16\n",
      "Waiting for Triton Server to be ready at triton:8000...\n",
      "200\n",
      ".................................................................Triton Server is ready!\n",
      "WARNING: Overriding max_threads specification to ensure requested concurrency range.\n",
      "*** Measurement Settings ***\n",
      "  Batch size: 8\n",
      "  Measurement window: 3000 msec\n",
      "  Latency limit: 500 msec\n",
      "  Concurrency limit: 10 concurrent requests\n",
      "  Using synchronous calls for inference\n",
      "  Stabilizing using average latency\n",
      "\n",
      "Request concurrency: 1\n",
      "  Pass [1] throughput: 285.333 infer/sec. Avg latency: 28127 usec (std 117 usec)\n",
      "  Pass [2] throughput: 282.667 infer/sec. Avg latency: 28220 usec (std 140 usec)\n",
      "  Pass [3] throughput: 285.333 infer/sec. Avg latency: 28217 usec (std 122 usec)\n",
      "  Client: \n",
      "    Request count: 107\n",
      "    Throughput: 285.333 infer/sec\n",
      "    Avg latency: 28217 usec (standard deviation 122 usec)\n",
      "    p50 latency: 28222 usec\n",
      "    p90 latency: 28329 usec\n",
      "    p95 latency: 28376 usec\n",
      "    p99 latency: 28580 usec\n",
      "    Avg HTTP time: 28212 usec (send 9 usec + response wait 28202 usec + receive 1 usec)\n",
      "  Server: \n",
      "    Inference count: 1024\n",
      "    Execution count: 128\n",
      "    Successful request count: 128\n",
      "    Avg request latency: 27840 usec (overhead 3 usec + queue 27 usec + compute input 17 usec + compute infer 27778 usec + compute output 15 usec)\n",
      "\n",
      "Request concurrency: 2\n",
      "  Pass [1] throughput: 285.333 infer/sec. Avg latency: 55983 usec (std 245 usec)\n",
      "  Pass [2] throughput: 285.333 infer/sec. Avg latency: 56054 usec (std 239 usec)\n",
      "  Pass [3] throughput: 288 infer/sec. Avg latency: 56039 usec (std 195 usec)\n",
      "  Client: \n",
      "    Request count: 108\n",
      "    Throughput: 288 infer/sec\n",
      "    Avg latency: 56039 usec (standard deviation 195 usec)\n",
      "    p50 latency: 56041 usec\n",
      "    p90 latency: 56281 usec\n",
      "    p95 latency: 56395 usec\n",
      "    p99 latency: 56555 usec\n",
      "    Avg HTTP time: 56027 usec (send 9 usec + response wait 56017 usec + receive 1 usec)\n",
      "  Server: \n",
      "    Inference count: 1024\n",
      "    Execution count: 128\n",
      "    Successful request count: 128\n",
      "    Avg request latency: 55638 usec (overhead 3 usec + queue 27704 usec + compute input 17 usec + compute infer 27901 usec + compute output 13 usec)\n",
      "\n",
      "Request concurrency: 3\n",
      "  Pass [1] throughput: 285.333 infer/sec. Avg latency: 84207 usec (std 359 usec)\n",
      "  Pass [2] throughput: 282.667 infer/sec. Avg latency: 84423 usec (std 338 usec)\n",
      "  Pass [3] throughput: 282.667 infer/sec. Avg latency: 84269 usec (std 337 usec)\n",
      "  Client: \n",
      "    Request count: 106\n",
      "    Throughput: 282.667 infer/sec\n",
      "    Avg latency: 84269 usec (standard deviation 337 usec)\n",
      "    p50 latency: 84274 usec\n",
      "    p90 latency: 84825 usec\n",
      "    p95 latency: 84922 usec\n",
      "    p99 latency: 85074 usec\n",
      "    Avg HTTP time: 84264 usec (send 9 usec + response wait 84254 usec + receive 1 usec)\n",
      "  Server: \n",
      "    Inference count: 1032\n",
      "    Execution count: 129\n",
      "    Successful request count: 129\n",
      "    Avg request latency: 83850 usec (overhead 3 usec + queue 55842 usec + compute input 16 usec + compute infer 27974 usec + compute output 15 usec)\n",
      "\n",
      "Request concurrency: 4\n",
      "  Pass [1] throughput: 285.333 infer/sec. Avg latency: 112243 usec (std 326 usec)\n",
      "  Pass [2] throughput: 282.667 infer/sec. Avg latency: 112647 usec (std 446 usec)\n",
      "  Pass [3] throughput: 282.667 infer/sec. Avg latency: 112647 usec (std 370 usec)\n",
      "  Client: \n",
      "    Request count: 106\n",
      "    Throughput: 282.667 infer/sec\n",
      "    Avg latency: 112647 usec (standard deviation 370 usec)\n",
      "    p50 latency: 112545 usec\n",
      "    p90 latency: 113134 usec\n",
      "    p95 latency: 113416 usec\n",
      "    p99 latency: 113795 usec\n",
      "    Avg HTTP time: 112632 usec (send 9 usec + response wait 112622 usec + receive 1 usec)\n",
      "  Server: \n",
      "    Inference count: 1024\n",
      "    Execution count: 128\n",
      "    Successful request count: 128\n",
      "    Avg request latency: 112222 usec (overhead 3 usec + queue 84148 usec + compute input 17 usec + compute infer 28042 usec + compute output 12 usec)\n",
      "\n",
      "Request concurrency: 5\n",
      "  Pass [1] throughput: 285.333 infer/sec. Avg latency: 141105 usec (std 409 usec)\n",
      "  Pass [2] throughput: 285.333 infer/sec. Avg latency: 141138 usec (std 537 usec)\n",
      "  Pass [3] throughput: 285.333 infer/sec. Avg latency: 140948 usec (std 360 usec)\n",
      "  Client: \n",
      "    Request count: 107\n",
      "    Throughput: 285.333 infer/sec\n",
      "    Avg latency: 140948 usec (standard deviation 360 usec)\n",
      "    p50 latency: 140892 usec\n",
      "    p90 latency: 141329 usec\n",
      "    p95 latency: 141457 usec\n",
      "    p99 latency: 142223 usec\n",
      "    Avg HTTP time: 140939 usec (send 9 usec + response wait 140929 usec + receive 1 usec)\n",
      "  Server: \n",
      "    Inference count: 1024\n",
      "    Execution count: 128\n",
      "    Successful request count: 128\n",
      "    Avg request latency: 140532 usec (overhead 3 usec + queue 112424 usec + compute input 17 usec + compute infer 28076 usec + compute output 12 usec)\n",
      "\n",
      "Request concurrency: 6\n",
      "  Pass [1] throughput: 282.667 infer/sec. Avg latency: 169215 usec (std 425 usec)\n",
      "  Pass [2] throughput: 282.667 infer/sec. Avg latency: 169299 usec (std 543 usec)\n",
      "  Pass [3] throughput: 285.333 infer/sec. Avg latency: 169468 usec (std 452 usec)\n",
      "  Client: \n",
      "    Request count: 107\n",
      "    Throughput: 285.333 infer/sec\n",
      "    Avg latency: 169468 usec (standard deviation 452 usec)\n",
      "    p50 latency: 169459 usec\n",
      "    p90 latency: 170093 usec\n",
      "    p95 latency: 170263 usec\n",
      "    p99 latency: 170376 usec\n",
      "    Avg HTTP time: 169484 usec (send 9 usec + response wait 169474 usec + receive 1 usec)\n",
      "  Server: \n",
      "    Inference count: 1016\n",
      "    Execution count: 127\n",
      "    Successful request count: 127\n",
      "    Avg request latency: 169072 usec (overhead 3 usec + queue 140908 usec + compute input 17 usec + compute infer 28132 usec + compute output 12 usec)\n",
      "\n",
      "Request concurrency: 7\n",
      "  Pass [1] throughput: 282.667 infer/sec. Avg latency: 197850 usec (std 619 usec)\n",
      "  Pass [2] throughput: 282.667 infer/sec. Avg latency: 197960 usec (std 637 usec)\n",
      "  Pass [3] throughput: 282.667 infer/sec. Avg latency: 198008 usec (std 598 usec)\n",
      "  Client: \n",
      "    Request count: 106\n",
      "    Throughput: 282.667 infer/sec\n",
      "    Avg latency: 198008 usec (standard deviation 598 usec)\n",
      "    p50 latency: 197949 usec\n",
      "    p90 latency: 198928 usec\n",
      "    p95 latency: 199174 usec\n",
      "    p99 latency: 199372 usec\n",
      "    Avg HTTP time: 198074 usec (send 11 usec + response wait 198062 usec + receive 1 usec)\n",
      "  Server: \n",
      "    Inference count: 1016\n",
      "    Execution count: 127\n",
      "    Successful request count: 127\n",
      "    Avg request latency: 197658 usec (overhead 4 usec + queue 169447 usec + compute input 17 usec + compute infer 28178 usec + compute output 12 usec)\n",
      "\n",
      "Request concurrency: 8\n",
      "  Pass [1] throughput: 280 infer/sec. Avg latency: 226450 usec (std 444 usec)\n",
      "  Pass [2] throughput: 280 infer/sec. Avg latency: 226612 usec (std 580 usec)\n",
      "  Pass [3] throughput: 280 infer/sec. Avg latency: 226654 usec (std 1493 usec)\n",
      "  Client: \n",
      "    Request count: 105\n",
      "    Throughput: 280 infer/sec\n",
      "    Avg latency: 226654 usec (standard deviation 1493 usec)\n",
      "    p50 latency: 226286 usec\n",
      "    p90 latency: 227161 usec\n",
      "    p95 latency: 231256 usec\n",
      "    p99 latency: 232359 usec\n",
      "    Avg HTTP time: 226649 usec (send 10 usec + response wait 226638 usec + receive 1 usec)\n",
      "  Server: \n",
      "    Inference count: 1016\n",
      "    Execution count: 127\n",
      "    Successful request count: 127\n",
      "    Avg request latency: 226236 usec (overhead 4 usec + queue 197995 usec + compute input 16 usec + compute infer 28209 usec + compute output 12 usec)\n",
      "\n",
      "Request concurrency: 9\n",
      "  Pass [1] throughput: 280 infer/sec. Avg latency: 254396 usec (std 2923 usec)\n",
      "  Pass [2] throughput: 282.667 infer/sec. Avg latency: 255558 usec (std 615 usec)\n",
      "  Pass [3] throughput: 282.667 infer/sec. Avg latency: 255827 usec (std 498 usec)\n",
      "  Client: \n",
      "    Request count: 106\n",
      "    Throughput: 282.667 infer/sec\n",
      "    Avg latency: 255827 usec (standard deviation 498 usec)\n",
      "    p50 latency: 255860 usec\n",
      "    p90 latency: 256485 usec\n",
      "    p95 latency: 256625 usec\n",
      "    p99 latency: 256955 usec\n",
      "    Avg HTTP time: 255725 usec (send 14 usec + response wait 255710 usec + receive 1 usec)\n",
      "  Server: \n",
      "    Inference count: 1016\n",
      "    Execution count: 127\n",
      "    Successful request count: 127\n",
      "    Avg request latency: 255293 usec (overhead 3 usec + queue 226964 usec + compute input 18 usec + compute infer 28296 usec + compute output 12 usec)\n",
      "\n",
      "Request concurrency: 10\n",
      "  Pass [1] throughput: 282.667 infer/sec. Avg latency: 282593 usec (std 4593 usec)\n",
      "  Pass [2] throughput: 280 infer/sec. Avg latency: 283498 usec (std 736 usec)\n",
      "  Pass [3] throughput: 280 infer/sec. Avg latency: 283667 usec (std 407 usec)\n",
      "  Client: \n",
      "    Request count: 105\n",
      "    Throughput: 280 infer/sec\n",
      "    Avg latency: 283667 usec (standard deviation 407 usec)\n",
      "    p50 latency: 283611 usec\n",
      "    p90 latency: 284289 usec\n",
      "    p95 latency: 284350 usec\n",
      "    p99 latency: 284485 usec\n",
      "    Avg HTTP time: 283644 usec (send 14 usec + response wait 283629 usec + receive 1 usec)\n",
      "  Server: \n",
      "    Inference count: 1016\n",
      "    Execution count: 127\n",
      "    Successful request count: 127\n",
      "    Avg request latency: 283208 usec (overhead 2 usec + queue 254927 usec + compute input 18 usec + compute infer 28248 usec + compute output 13 usec)\n",
      "\n",
      "Inferences/Second vs. Client Average Batch Latency\n",
      "Concurrency: 1, throughput: 285.333 infer/sec, latency 28217 usec\n",
      "Concurrency: 2, throughput: 288 infer/sec, latency 56039 usec\n",
      "Concurrency: 3, throughput: 282.667 infer/sec, latency 84269 usec\n",
      "Concurrency: 4, throughput: 282.667 infer/sec, latency 112647 usec\n",
      "Concurrency: 5, throughput: 285.333 infer/sec, latency 140948 usec\n",
      "Concurrency: 6, throughput: 285.333 infer/sec, latency 169468 usec\n",
      "Concurrency: 7, throughput: 282.667 infer/sec, latency 198008 usec\n",
      "Concurrency: 8, throughput: 280 infer/sec, latency 226654 usec\n",
      "Concurrency: 9, throughput: 282.667 infer/sec, latency 255827 usec\n",
      "Concurrency: 10, throughput: 280 infer/sec, latency 283667 usec\n"
     ]
    }
   ],
   "source": [
    "modelName = \"bertQA-onnx-trt-fp16\"\n",
    "maxConcurrency= \"10\"\n",
    "batchSize=\"8\"\n",
    "print(\"Running: \" + modelName)\n",
    "!bash ./utilities/run_perf_client_local.sh \\\n",
    "                    {modelName} \\\n",
    "                    {modelVersion} \\\n",
    "                    {precision} \\\n",
    "                    {batchSize} \\\n",
    "                    {maxLatency} \\\n",
    "                    {maxClientThreads} \\\n",
    "                    {maxConcurrency} \\\n",
    "                    {tritonServerHostName} \\\n",
    "                    {dockerBridge} \\\n",
    "                    {resultsFolderName} \\\n",
    "                    {profilingData}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.5 Performance Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's compare the performance against ONNX runtime. \n",
    "* How did the latency change, especially across larger concurrency runs? \n",
    "* How did the bandwidth change? Can you explain the level of bandwidth change observed? \n",
    "* Why did the ONNX model timeout at concurrency of less than 10? How does the TensorRT latency at concurrency 10 compare to latency of pure ONNX runtime at an earlier concurrency?\n",
    "\n",
    "Discuss with the instructor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:green;\">Congratulations!</h3><br>\n",
    "You've successfully deployed an NLP model to Triton Server with TorchScript and applied both reduced precision and TensorRT optimizations.\n",
    "In the next notebook you'll learn how to optimize the model itself and to deploy it in an efficient way. \n",
    "\n",
    "Please proceed to the next notebook:<br>\n",
    "[2.0 Hosting the model](020_HostingTheModel.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
