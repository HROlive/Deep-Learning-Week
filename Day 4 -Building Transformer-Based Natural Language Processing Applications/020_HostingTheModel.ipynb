{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0 Hosting the Model\n",
    "\n",
    "In this notebook, you'll learn strategies to optimize Triton Server to improve the performance of your deployment.\n",
    "\n",
    "\n",
    "**[2.1 Concurrent Model Execution](#2.1-Concurrent-Model-Execution)**<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; [2.1.1 Exercise: Usage Considerations](#2.1.1-Exercise:-Usage-Considerations)<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; [2.1.2 Implementation](#2.1.2-Implementation)<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; [2.1.3 Exercise: Configure Multiple Instance Groups](#2.1.3-Exercise:-Configure-Multiple-Instance-Groups)<br>\n",
    "**[2.2 Scheduling Strategies](#2.2-Scheduling-Strategies)**<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; [2.2.1 Stateless Inference](#2.2.1-Stateless-Inference)<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; [2.2.2 Stateful Inference](#2.2.2-Stateful-Inference)<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; [2.2.3 Pipelines / Ensembles](#2.2.3-Pipelines-/-Ensembles)<br>\n",
    "**[2.3 Dynamic Batching](#2.3-Dynamic-Batching)**<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; [2.3.1 Exercise: Implement Dynamic Batching](#2.3.1-Exercise:-Implement-Dynamic-Batching)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we've executed customer requests sequentially, in the order they have arrived at the server, and used a static batch of size 8 for any requests to our server. This has not only left our GPUs heavily underutilized, but has also significantly affected the latency of responses received from the server. This is not an uncommon situation. Unless you are developing an application that processes large volumes of data in batch, you will likely be sending individual inference requests from the user application, leading to even further underutilization. As we have seen in the previous notebook, model optimizations do help considerably to accelerate model execution.  However, they do not change the fact that when serving is implemented naively, the nature of the inference workload leads to GPU underutilization.\n",
    "\n",
    "Inference servers, such as NVIDIA Triton, implement a wide range of features that allow us to improve the GPU utilization and improve request latency. The three that we will discuss in this class are:<br/>\n",
    "- <a href=\"https://docs.nvidia.com/deeplearning/triton-inference-server/master-user-guide/docs/architecture.html#section-concurrent-model-execution\">Concurrent model execution</a></br>\n",
    "- <a href=\"https://docs.nvidia.com/deeplearning/triton-inference-server/master-user-guide/docs/models_and_schedulers.html\">Scheduling</a> <br/>\n",
    "- <a href=\"https://docs.nvidia.com/deeplearning/triton-inference-server/master-user-guide/docs/model_configuration.html#section-dynamic-batcher\">Dynamic batching</a> <br/>\n",
    "\n",
    "\n",
    "Please refer to the <a href=\"https://docs.nvidia.com/deeplearning/triton-inference-server/master-user-guide/docs/quickstart.html\">Triton documentation</a> and its <a href=\"https://github.com/NVIDIA/triton-inference-server\">source code</a> for further information about the mechanisms and configurations that can help improve model inference performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Concurrent Model Execution\n",
    "The Triton architecture allows multiple models and/or multiple instances of the same model to execute in parallel on a single GPU. The following figure shows an example with two models: `model0` and `model1`. Assuming Triton is not currently processing any request, when two requests arrive simultaneously, one for each model, Triton immediately schedules both of them onto the GPU, and the GPUâ€™s hardware scheduler begins working on both computations in parallel. </br>\n",
    "\n",
    "<img src=\"images/multi_model_exec.png\"/><br/>\n",
    "\n",
    "#### Default Behavior\n",
    "\n",
    "By default, if multiple requests for the same model arrive at the same time, Triton will serialize their execution by scheduling only one at a time on the GPU, as shown in the following figure.\n",
    "\n",
    "<img src=\"images/multi_model_serial_exec.png\"/><br/>\n",
    "\n",
    "Triton provides an instance-group feature that allows each model to specify how many parallel executions of that model should be allowed. Each such enabled parallel execution is referred to as an *execution instance*. By default, Triton gives each model a single execution instance, which means that only a single execution of the model is allowed to be in progress at a time as shown in the above figure. \n",
    "\n",
    "#### Instance Groups\n",
    "By using the *instance-group* setting, the number of execution instances for a model can be increased. The following figure shows model execution when `model1` is configured to allow three execution instances. As shown in the figure, the first three `model1` inference requests are immediately executed in parallel on the GPU. The fourth `model1` inference request must wait until one of the first three executions completes before beginning.\n",
    "\n",
    "<img src=\"images/multi_model_parallel_exec.png\"/><br/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.1 Exercise: Usage Considerations\n",
    "\n",
    "For most models, the Triton feature that provides the largest performance improvement is *dynamic batching*. The key advantages of dynamic batching over setting up multiple instance execution are:\n",
    "- No overhead for model parameter storage\n",
    "- No overhead related to model parameter fetch from the GPU memory\n",
    "- Better utilization of the GPU resources\n",
    "\n",
    "Before we look at the configuration for multiple model execution, let's execute our model again using a single instance, and observe the resource utilization of the GPU. <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise Steps\n",
    "1. Launch a terminal window from the JupyterLab launch page.  If you need to open a new launch page, click the '+' icon on the left sidebar menu. You can then use a drag-and-drop action to move the terminal to a sub-window configuration  for better viewing.\n",
    "2. Execute the following command in the terminal before you run the performance tool:<br>\n",
    "\n",
    "```\n",
    "watch -n0.5 nvidia-smi\n",
    "```\n",
    "    You should see an output that resembles:\n",
    "<img src=\"images/NVIDIASMI.png\" style=\"position:relative; left:30px;\" width=800/>\n",
    "\n",
    "3. Execute the same benchmark we used in the previous notebook, but with the batch size reduced to 1, and observe the <code>nvidia-smi</code> output again.  Pay special attention to the memory consumption and GPU utilization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for Triton Server to be ready at triton:8000...\n",
      "200\n",
      "Triton Server is ready!\n"
     ]
    }
   ],
   "source": [
    "# Set the server hostname and check it - you should get a message that \"Triton Server is ready!\"\n",
    "tritonServerHostName = \"triton\"\n",
    "!./utilities/wait_for_triton_server.sh {tritonServerHostName}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the previous configuration.\n",
    "modelVersion=\"1\"\n",
    "precision=\"fp32\"\n",
    "batchSize=\"1\"\n",
    "maxLatency=\"500\"\n",
    "maxClientThreads=\"10\"\n",
    "maxConcurrency=\"2\"\n",
    "dockerBridge=\"host\"\n",
    "resultsFolderName=\"1\"\n",
    "profilingData=\"utilities/profiling_data_int64\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: bertQA-onnx-trt-fp16\n",
      "Waiting for Triton Server to be ready at triton:8000...\n",
      "200\n",
      "Triton Server is ready!\n",
      "WARNING: Overriding max_threads specification to ensure requested concurrency range.\n",
      "*** Measurement Settings ***\n",
      "  Batch size: 1\n",
      "  Measurement window: 3000 msec\n",
      "  Latency limit: 500 msec\n",
      "  Concurrency limit: 10 concurrent requests\n",
      "  Using synchronous calls for inference\n",
      "  Stabilizing using average latency\n",
      "\n",
      "Request concurrency: 1\n",
      "  Pass [1] throughput: 35.6667 infer/sec. Avg latency: 28147 usec (std 169 usec)\n",
      "  Pass [2] throughput: 35.3333 infer/sec. Avg latency: 28206 usec (std 135 usec)\n",
      "  Pass [3] throughput: 35.6667 infer/sec. Avg latency: 28296 usec (std 149 usec)\n",
      "  Client: \n",
      "    Request count: 107\n",
      "    Throughput: 35.6667 infer/sec\n",
      "    Avg latency: 28296 usec (standard deviation 149 usec)\n",
      "    p50 latency: 28249 usec\n",
      "    p90 latency: 28509 usec\n",
      "    p95 latency: 28584 usec\n",
      "    p99 latency: 28671 usec\n",
      "    Avg HTTP time: 28287 usec (send 4 usec + response wait 28282 usec + receive 1 usec)\n",
      "  Server: \n",
      "    Inference count: 128\n",
      "    Execution count: 128\n",
      "    Successful request count: 128\n",
      "    Avg request latency: 27997 usec (overhead 3 usec + queue 37 usec + compute input 10 usec + compute infer 27937 usec + compute output 10 usec)\n",
      "\n",
      "Request concurrency: 2\n",
      "  Pass [1] throughput: 35.6667 infer/sec. Avg latency: 56117 usec (std 283 usec)\n",
      "  Pass [2] throughput: 35.6667 infer/sec. Avg latency: 56080 usec (std 207 usec)\n",
      "  Pass [3] throughput: 35.6667 infer/sec. Avg latency: 56053 usec (std 182 usec)\n",
      "  Client: \n",
      "    Request count: 107\n",
      "    Throughput: 35.6667 infer/sec\n",
      "    Avg latency: 56053 usec (standard deviation 182 usec)\n",
      "    p50 latency: 56050 usec\n",
      "    p90 latency: 56260 usec\n",
      "    p95 latency: 56343 usec\n",
      "    p99 latency: 56506 usec\n",
      "    Avg HTTP time: 56045 usec (send 4 usec + response wait 56040 usec + receive 1 usec)\n",
      "  Server: \n",
      "    Inference count: 129\n",
      "    Execution count: 129\n",
      "    Successful request count: 129\n",
      "    Avg request latency: 55720 usec (overhead 2 usec + queue 27777 usec + compute input 9 usec + compute infer 27922 usec + compute output 10 usec)\n",
      "\n",
      "Request concurrency: 3\n",
      "  Pass [1] throughput: 35.6667 infer/sec. Avg latency: 84131 usec (std 280 usec)\n",
      "  Pass [2] throughput: 35.6667 infer/sec. Avg latency: 84253 usec (std 254 usec)\n",
      "  Pass [3] throughput: 35.6667 infer/sec. Avg latency: 84422 usec (std 306 usec)\n",
      "  Client: \n",
      "    Request count: 107\n",
      "    Throughput: 35.6667 infer/sec\n",
      "    Avg latency: 84422 usec (standard deviation 306 usec)\n",
      "    p50 latency: 84414 usec\n",
      "    p90 latency: 84857 usec\n",
      "    p95 latency: 84944 usec\n",
      "    p99 latency: 85238 usec\n",
      "    Avg HTTP time: 84447 usec (send 4 usec + response wait 84442 usec + receive 1 usec)\n",
      "  Server: \n",
      "    Inference count: 128\n",
      "    Execution count: 128\n",
      "    Successful request count: 128\n",
      "    Avg request latency: 84095 usec (overhead 3 usec + queue 56026 usec + compute input 9 usec + compute infer 28047 usec + compute output 10 usec)\n",
      "\n",
      "Request concurrency: 4\n",
      "  Pass [1] throughput: 35.6667 infer/sec. Avg latency: 112578 usec (std 259 usec)\n",
      "  Pass [2] throughput: 35.3333 infer/sec. Avg latency: 112655 usec (std 333 usec)\n",
      "  Pass [3] throughput: 35.6667 infer/sec. Avg latency: 112791 usec (std 353 usec)\n",
      "  Client: \n",
      "    Request count: 107\n",
      "    Throughput: 35.6667 infer/sec\n",
      "    Avg latency: 112791 usec (standard deviation 353 usec)\n",
      "    p50 latency: 112707 usec\n",
      "    p90 latency: 113364 usec\n",
      "    p95 latency: 113533 usec\n",
      "    p99 latency: 113590 usec\n",
      "    Avg HTTP time: 112798 usec (send 5 usec + response wait 112792 usec + receive 1 usec)\n",
      "  Server: \n",
      "    Inference count: 127\n",
      "    Execution count: 127\n",
      "    Successful request count: 127\n",
      "    Avg request latency: 112441 usec (overhead 4 usec + queue 84322 usec + compute input 10 usec + compute infer 28095 usec + compute output 10 usec)\n",
      "\n",
      "Request concurrency: 5\n",
      "  Pass [1] throughput: 35.3333 infer/sec. Avg latency: 141021 usec (std 486 usec)\n",
      "  Pass [2] throughput: 35.6667 infer/sec. Avg latency: 141095 usec (std 435 usec)\n",
      "  Pass [3] throughput: 35.3333 infer/sec. Avg latency: 141150 usec (std 475 usec)\n",
      "  Client: \n",
      "    Request count: 106\n",
      "    Throughput: 35.3333 infer/sec\n",
      "    Avg latency: 141150 usec (standard deviation 475 usec)\n",
      "    p50 latency: 141060 usec\n",
      "    p90 latency: 141840 usec\n",
      "    p95 latency: 142079 usec\n",
      "    p99 latency: 142113 usec\n",
      "    Avg HTTP time: 141120 usec (send 5 usec + response wait 141114 usec + receive 1 usec)\n",
      "  Server: \n",
      "    Inference count: 127\n",
      "    Execution count: 127\n",
      "    Successful request count: 127\n",
      "    Avg request latency: 140764 usec (overhead 4 usec + queue 112618 usec + compute input 9 usec + compute infer 28123 usec + compute output 10 usec)\n",
      "\n",
      "Request concurrency: 6\n",
      "  Pass [1] throughput: 35.6667 infer/sec. Avg latency: 169499 usec (std 518 usec)\n",
      "  Pass [2] throughput: 35.6667 infer/sec. Avg latency: 169411 usec (std 485 usec)\n",
      "  Pass [3] throughput: 35.3333 infer/sec. Avg latency: 169773 usec (std 521 usec)\n",
      "  Client: \n",
      "    Request count: 106\n",
      "    Throughput: 35.3333 infer/sec\n",
      "    Avg latency: 169773 usec (standard deviation 521 usec)\n",
      "    p50 latency: 169765 usec\n",
      "    p90 latency: 170498 usec\n",
      "    p95 latency: 170717 usec\n",
      "    p99 latency: 171103 usec\n",
      "    Avg HTTP time: 169778 usec (send 6 usec + response wait 169771 usec + receive 1 usec)\n",
      "  Server: \n",
      "    Inference count: 128\n",
      "    Execution count: 128\n",
      "    Successful request count: 128\n",
      "    Avg request latency: 169392 usec (overhead 3 usec + queue 141189 usec + compute input 11 usec + compute infer 28177 usec + compute output 12 usec)\n",
      "\n",
      "Request concurrency: 7\n",
      "  Pass [1] throughput: 35.6667 infer/sec. Avg latency: 197919 usec (std 393 usec)\n",
      "  Pass [2] throughput: 35.3333 infer/sec. Avg latency: 198376 usec (std 481 usec)\n",
      "  Pass [3] throughput: 35.3333 infer/sec. Avg latency: 198382 usec (std 491 usec)\n",
      "  Client: \n",
      "    Request count: 106\n",
      "    Throughput: 35.3333 infer/sec\n",
      "    Avg latency: 198382 usec (standard deviation 491 usec)\n",
      "    p50 latency: 198383 usec\n",
      "    p90 latency: 199164 usec\n",
      "    p95 latency: 199279 usec\n",
      "    p99 latency: 199514 usec\n",
      "    Avg HTTP time: 198485 usec (send 5 usec + response wait 198479 usec + receive 1 usec)\n",
      "  Server: \n",
      "    Inference count: 127\n",
      "    Execution count: 127\n",
      "    Successful request count: 127\n",
      "    Avg request latency: 198140 usec (overhead 2 usec + queue 169872 usec + compute input 9 usec + compute infer 28246 usec + compute output 11 usec)\n",
      "\n",
      "Request concurrency: 8\n",
      "  Pass [1] throughput: 35.3333 infer/sec. Avg latency: 227098 usec (std 1130 usec)\n",
      "  Pass [2] throughput: 35 infer/sec. Avg latency: 226676 usec (std 433 usec)\n",
      "  Pass [3] throughput: 35 infer/sec. Avg latency: 226498 usec (std 313 usec)\n",
      "  Client: \n",
      "    Request count: 105\n",
      "    Throughput: 35 infer/sec\n",
      "    Avg latency: 226498 usec (standard deviation 313 usec)\n",
      "    p50 latency: 226451 usec\n",
      "    p90 latency: 226905 usec\n",
      "    p95 latency: 227131 usec\n",
      "    p99 latency: 227343 usec\n",
      "    Avg HTTP time: 226499 usec (send 6 usec + response wait 226492 usec + receive 1 usec)\n",
      "  Server: \n",
      "    Inference count: 127\n",
      "    Execution count: 127\n",
      "    Successful request count: 127\n",
      "    Avg request latency: 226136 usec (overhead 2 usec + queue 197902 usec + compute input 10 usec + compute infer 28211 usec + compute output 11 usec)\n",
      "\n",
      "Request concurrency: 9\n",
      "  Pass [1] throughput: 35 infer/sec. Avg latency: 254677 usec (std 3076 usec)\n",
      "  Pass [2] throughput: 35.3333 infer/sec. Avg latency: 255233 usec (std 406 usec)\n",
      "  Pass [3] throughput: 35.3333 infer/sec. Avg latency: 255358 usec (std 471 usec)\n",
      "  Client: \n",
      "    Request count: 106\n",
      "    Throughput: 35.3333 infer/sec\n",
      "    Avg latency: 255358 usec (standard deviation 471 usec)\n",
      "    p50 latency: 255358 usec\n",
      "    p90 latency: 255972 usec\n",
      "    p95 latency: 256123 usec\n",
      "    p99 latency: 256480 usec\n",
      "    Avg HTTP time: 255307 usec (send 6 usec + response wait 255300 usec + receive 1 usec)\n",
      "  Server: \n",
      "    Inference count: 127\n",
      "    Execution count: 127\n",
      "    Successful request count: 127\n",
      "    Avg request latency: 254947 usec (overhead 3 usec + queue 226661 usec + compute input 10 usec + compute infer 28262 usec + compute output 11 usec)\n",
      "\n",
      "Request concurrency: 10\n",
      "  Pass [1] throughput: 35.3333 infer/sec. Avg latency: 283087 usec (std 4840 usec)\n",
      "  Pass [2] throughput: 35 infer/sec. Avg latency: 283721 usec (std 438 usec)\n",
      "  Pass [3] throughput: 35.3333 infer/sec. Avg latency: 283908 usec (std 499 usec)\n",
      "  Client: \n",
      "    Request count: 106\n",
      "    Throughput: 35.3333 infer/sec\n",
      "    Avg latency: 283908 usec (standard deviation 499 usec)\n",
      "    p50 latency: 283876 usec\n",
      "    p90 latency: 284398 usec\n",
      "    p95 latency: 285040 usec\n",
      "    p99 latency: 285388 usec\n",
      "    Avg HTTP time: 283940 usec (send 7 usec + response wait 283932 usec + receive 1 usec)\n",
      "  Server: \n",
      "    Inference count: 126\n",
      "    Execution count: 126\n",
      "    Successful request count: 126\n",
      "    Avg request latency: 283562 usec (overhead 4 usec + queue 255251 usec + compute input 10 usec + compute infer 28286 usec + compute output 11 usec)\n",
      "\n",
      "Inferences/Second vs. Client Average Batch Latency\n",
      "Concurrency: 1, throughput: 35.6667 infer/sec, latency 28296 usec\n",
      "Concurrency: 2, throughput: 35.6667 infer/sec, latency 56053 usec\n",
      "Concurrency: 3, throughput: 35.6667 infer/sec, latency 84422 usec\n",
      "Concurrency: 4, throughput: 35.6667 infer/sec, latency 112791 usec\n",
      "Concurrency: 5, throughput: 35.3333 infer/sec, latency 141150 usec\n",
      "Concurrency: 6, throughput: 35.3333 infer/sec, latency 169773 usec\n",
      "Concurrency: 7, throughput: 35.3333 infer/sec, latency 198382 usec\n",
      "Concurrency: 8, throughput: 35 infer/sec, latency 226498 usec\n",
      "Concurrency: 9, throughput: 35.3333 infer/sec, latency 255358 usec\n",
      "Concurrency: 10, throughput: 35.3333 infer/sec, latency 283908 usec\n"
     ]
    }
   ],
   "source": [
    "# Update configuration parameters and run profiler.\n",
    "modelName = \"bertQA-onnx-trt-fp16\"\n",
    "maxConcurrency= \"10\"\n",
    "batchSize=\"1\"\n",
    "print(\"Running: \" + modelName)\n",
    "!bash ./utilities/run_perf_client_local.sh \\\n",
    "                    {modelName} \\\n",
    "                    {modelVersion} \\\n",
    "                    {precision} \\\n",
    "                    {batchSize} \\\n",
    "                    {maxLatency} \\\n",
    "                    {maxClientThreads} \\\n",
    "                    {maxConcurrency} \\\n",
    "                    {tritonServerHostName} \\\n",
    "                    {dockerBridge} \\\n",
    "                    {resultsFolderName} \\\n",
    "                    {profilingData}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully, you have observed utilization similar to the following:<br/>\n",
    "<img src=\"images/NVIDIASMI2.png\" width=800/><br/>\n",
    "\n",
    "Do you think you will observe a major acceleration as a consequence of increasing the number of instance groups?<br>\n",
    "Discuss with the instructor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.2 Implementation\n",
    "Let's look at how to enable concurrent execution and what impact it will have on our model performance. Execute the following code cells to export the model in the ONNX format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelName = \"bertQA-onnx-conexec\"\n",
    "exportFormat = \"onnx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deploying model bertQA-onnx-conexec in format onnxruntime_onnx\n",
      "/opt/conda/lib/python3.6/site-packages/torch/onnx/utils.py:955: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input input__0\n",
      "  'Automatically generated names will be applied to each dynamic axes of input {}'.format(key))\n",
      "/opt/conda/lib/python3.6/site-packages/torch/onnx/utils.py:955: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input input__1\n",
      "  'Automatically generated names will be applied to each dynamic axes of input {}'.format(key))\n",
      "/opt/conda/lib/python3.6/site-packages/torch/onnx/utils.py:955: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input input__2\n",
      "  'Automatically generated names will be applied to each dynamic axes of input {}'.format(key))\n",
      "/opt/conda/lib/python3.6/site-packages/torch/onnx/utils.py:955: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input output__0\n",
      "  'Automatically generated names will be applied to each dynamic axes of input {}'.format(key))\n",
      "/opt/conda/lib/python3.6/site-packages/torch/onnx/utils.py:955: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input output__1\n",
      "  'Automatically generated names will be applied to each dynamic axes of input {}'.format(key))\n",
      "[libprotobuf WARNING google/protobuf/io/coded_stream.cc:604] Reading dangerously large protocol message.  If the message turns out to be larger than 2147483647 bytes, parsing will be halted for security reasons.  To increase the limit (or to disable these warnings), see CodedInputStream::SetTotalBytesLimit() in google/protobuf/io/coded_stream.h.\n",
      "[libprotobuf WARNING google/protobuf/io/coded_stream.cc:81] The total number of bytes read was 1336539973\n",
      "\n",
      "conversion correctness test results\n",
      "-----------------------------------\n",
      "maximal absolute error over dataset (L_inf):  0.00022935867309570312\n",
      "\n",
      "average L_inf error over output tensors:  0.0001423954963684082\n",
      "variance of L_inf error over output tensors:  6.657553323445124e-09\n",
      "stddev of L_inf error over output tensors:  8.159383140559784e-05\n",
      "\n",
      "time of error check of native model:  0.4151346683502197 seconds\n",
      "time of error check of onnx model:  19.021291971206665 seconds\n",
      "\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "!python ./deployer/deployer.py \\\n",
    "    --{exportFormat} \\\n",
    "    --save-dir ./candidatemodels \\\n",
    "    --triton-model-name {modelName} \\\n",
    "    --triton-model-version 1 \\\n",
    "    --triton-max-batch-size 8 \\\n",
    "    --triton-dyn-batching-delay 0 \\\n",
    "    --triton-engine-count 1 \\\n",
    "    -- --checkpoint ./data/bert_qa.pt \\\n",
    "    --config_file ./bert_config.json \\\n",
    "    --vocab_file ./vocab \\\n",
    "    --predict_file ./squad/v1.1/dev-v1.1.json \\\n",
    "    --do_lower_case \\\n",
    "    --batch_size=8 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 16K\n",
      "drwxr-xr-x 3 root root 4.0K Jul 28 12:56 .\n",
      "drwxr-xr-x 3 root root 4.0K Jul 28 12:56 ..\n",
      "drwxr-xr-x 2 root root 4.0K Jul 28 12:56 1\n",
      "-rw-r--r-- 1 root root  569 Jul 28 12:56 config.pbtxt\n"
     ]
    }
   ],
   "source": [
    "!ls -alh ./candidatemodels/bertQA-onnx-conexec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.3 Exercise: Configure Multiple Instance Groups\n",
    "In order to specify multiple instances, we need to change the \"count\" value from '1' to a larger number in the `instance_group` section of the \"config.pbtxt\" configuration file. \n",
    "\n",
    "\n",
    "```\n",
    "    instance_group [\n",
    "    {\n",
    "        count: 2\n",
    "        kind: KIND_GPU\n",
    "        gpus: [ 0 ]\n",
    "    }\n",
    "]\n",
    "```\n",
    "\n",
    "#### Exercise Steps:\n",
    "1. Modify [config.pbtxt](candidatemodels/bertQA-onnx-conexec/config.pbtxt) in the `bertQA-onnx-conexec` deployment just created to specify two instances of our BERT-based question answering model. You should find the default instance_group block at the end of the file. Change the count variable from 1 to 2.  (see the [solution](solutions/ex-2-1-3_config.pbtxt) as needed)\n",
    "2. To make the comparison fair, also enable TensorRT with the addition of an `execution_accelerators` block inside the `optimization` block:\n",
    "\n",
    "```text\n",
    "optimization {\n",
    "   execution_accelerators {\n",
    "      gpu_execution_accelerator : [ {\n",
    "         name : \"tensorrt\"\n",
    "         parameters { key: \"precision_mode\" value: \"FP16\" }\n",
    "      }]\n",
    "   }\n",
    "cuda { graphs: 0 }\n",
    "}\n",
    "```\n",
    "\n",
    "3. Once you have saved your changes (Main menu: File -> Save File), move the model across to Triton by executing the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config.pbxt code\n",
    "name: \"bertQA-onnx-conexec\"\n",
    "platform: \"onnxruntime_onnx\"\n",
    "max_batch_size: 8\n",
    "input [\n",
    "{\n",
    "    name: \"input__0\"\n",
    "    data_type: TYPE_INT64\n",
    "    dims: [384]\n",
    "},\n",
    "{\n",
    "    name: \"input__1\"\n",
    "    data_type: TYPE_INT64\n",
    "    dims: [384]\n",
    "},\n",
    "{\n",
    "    name: \"input__2\"\n",
    "    data_type: TYPE_INT64\n",
    "    dims: [384]\n",
    "}\n",
    "]\n",
    "output [\n",
    "{\n",
    "    name: \"output__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [384]\n",
    "}, \n",
    "{\n",
    "    name: \"output__1\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [384]\n",
    "}\n",
    "]\n",
    "optimization {\n",
    "   execution_accelerators {\n",
    "      gpu_execution_accelerator : [ {\n",
    "         name : \"tensorrt\"\n",
    "         parameters { key: \"precision_mode\" value: \"FP16\" }\n",
    "      }]\n",
    "   }\n",
    "cuda { graphs: 0 }\n",
    "}\n",
    "instance_group [\n",
    "    {\n",
    "        count: 2\n",
    "        kind: KIND_GPU\n",
    "        gpus: [ 0 ]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv ./candidatemodels/bertQA-onnx-conexec model_repository/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Run our standard stress test against the model. Please compare it to the single instance execution.<br>\n",
    "   Did the throughput change?<br>\n",
    "   Did the latency change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: bertQA-onnx-conexec\n",
      "Waiting for Triton Server to be ready at triton:8000...\n",
      "200\n",
      "........................................................Triton Server is ready!\n",
      "WARNING: Overriding max_threads specification to ensure requested concurrency range.\n",
      "*** Measurement Settings ***\n",
      "  Batch size: 1\n",
      "  Measurement window: 3000 msec\n",
      "  Latency limit: 500 msec\n",
      "  Concurrency limit: 10 concurrent requests\n",
      "  Using synchronous calls for inference\n",
      "  Stabilizing using average latency\n",
      "\n",
      "Request concurrency: 1\n",
      "  Pass [1] throughput: 35.3333 infer/sec. Avg latency: 28198 usec (std 432 usec)\n",
      "  Pass [2] throughput: 35.6667 infer/sec. Avg latency: 28234 usec (std 115 usec)\n",
      "  Pass [3] throughput: 35.3333 infer/sec. Avg latency: 28263 usec (std 124 usec)\n",
      "  Client: \n",
      "    Request count: 106\n",
      "    Throughput: 35.3333 infer/sec\n",
      "    Avg latency: 28263 usec (standard deviation 124 usec)\n",
      "    p50 latency: 28249 usec\n",
      "    p90 latency: 28434 usec\n",
      "    p95 latency: 28501 usec\n",
      "    p99 latency: 28604 usec\n",
      "    Avg HTTP time: 28254 usec (send 5 usec + response wait 28248 usec + receive 1 usec)\n",
      "  Server: \n",
      "    Inference count: 127\n",
      "    Execution count: 127\n",
      "    Successful request count: 127\n",
      "    Avg request latency: 27937 usec (overhead 3 usec + queue 19 usec + compute input 10 usec + compute infer 27891 usec + compute output 14 usec)\n",
      "\n",
      "Request concurrency: 2\n",
      "  Pass [1] throughput: 35.6667 infer/sec. Avg latency: 56011 usec (std 1692 usec)\n",
      "  Pass [2] throughput: 35.6667 infer/sec. Avg latency: 56025 usec (std 1406 usec)\n",
      "  Pass [3] throughput: 35.6667 infer/sec. Avg latency: 56062 usec (std 2287 usec)\n",
      "  Client: \n",
      "    Request count: 107\n",
      "    Throughput: 35.6667 infer/sec\n",
      "    Avg latency: 56062 usec (standard deviation 2287 usec)\n",
      "    p50 latency: 55859 usec\n",
      "    p90 latency: 59071 usec\n",
      "    p95 latency: 59647 usec\n",
      "    p99 latency: 61308 usec\n",
      "    Avg HTTP time: 56069 usec (send 5 usec + response wait 56063 usec + receive 1 usec)\n",
      "  Server: \n",
      "    Inference count: 129\n",
      "    Execution count: 129\n",
      "    Successful request count: 129\n",
      "    Avg request latency: 55712 usec (overhead 4 usec + queue 36 usec + compute input 10 usec + compute infer 55649 usec + compute output 13 usec)\n",
      "\n",
      "Request concurrency: 3\n",
      "  Pass [1] throughput: 35.3333 infer/sec. Avg latency: 84065 usec (std 9938 usec)\n",
      "  Pass [2] throughput: 35.3333 infer/sec. Avg latency: 84449 usec (std 16401 usec)\n",
      "  Pass [3] throughput: 35.6667 infer/sec. Avg latency: 84408 usec (std 19540 usec)\n",
      "  Client: \n",
      "    Request count: 107\n",
      "    Throughput: 35.6667 infer/sec\n",
      "    Avg latency: 84408 usec (standard deviation 19540 usec)\n",
      "    p50 latency: 83936 usec\n",
      "    p90 latency: 110374 usec\n",
      "    p95 latency: 111592 usec\n",
      "    p99 latency: 112383 usec\n",
      "    Avg HTTP time: 84382 usec (send 6 usec + response wait 84375 usec + receive 1 usec)\n",
      "  Server: \n",
      "    Inference count: 127\n",
      "    Execution count: 127\n",
      "    Successful request count: 127\n",
      "    Avg request latency: 83987 usec (overhead 3 usec + queue 27827 usec + compute input 12 usec + compute infer 56127 usec + compute output 18 usec)\n",
      "\n",
      "Request concurrency: 4\n",
      "  Pass [1] throughput: 35.6667 infer/sec. Avg latency: 112655 usec (std 2889 usec)\n",
      "  Pass [2] throughput: 35.3333 infer/sec. Avg latency: 112647 usec (std 1724 usec)\n",
      "  Pass [3] throughput: 35.3333 infer/sec. Avg latency: 112552 usec (std 1599 usec)\n",
      "  Client: \n",
      "    Request count: 106\n",
      "    Throughput: 35.3333 infer/sec\n",
      "    Avg latency: 112552 usec (standard deviation 1599 usec)\n",
      "    p50 latency: 112569 usec\n",
      "    p90 latency: 114779 usec\n",
      "    p95 latency: 115229 usec\n",
      "    p99 latency: 115497 usec\n",
      "    Avg HTTP time: 112511 usec (send 6 usec + response wait 112503 usec + receive 2 usec)\n",
      "  Server: \n",
      "    Inference count: 128\n",
      "    Execution count: 128\n",
      "    Successful request count: 128\n",
      "    Avg request latency: 112115 usec (overhead 3 usec + queue 55908 usec + compute input 11 usec + compute infer 56180 usec + compute output 13 usec)\n",
      "\n",
      "Request concurrency: 5\n",
      "  Pass [1] throughput: 35.6667 infer/sec. Avg latency: 141055 usec (std 12622 usec)\n",
      "  Pass [2] throughput: 35.6667 infer/sec. Avg latency: 141112 usec (std 17689 usec)\n",
      "  Pass [3] throughput: 35 infer/sec. Avg latency: 141452 usec (std 13385 usec)\n",
      "  Client: \n",
      "    Request count: 105\n",
      "    Throughput: 35 infer/sec\n",
      "    Avg latency: 141452 usec (standard deviation 13385 usec)\n",
      "    p50 latency: 141185 usec\n",
      "    p90 latency: 159931 usec\n",
      "    p95 latency: 161476 usec\n",
      "    p99 latency: 164031 usec\n",
      "    Avg HTTP time: 141419 usec (send 6 usec + response wait 141412 usec + receive 1 usec)\n",
      "  Server: \n",
      "    Inference count: 127\n",
      "    Execution count: 127\n",
      "    Successful request count: 127\n",
      "    Avg request latency: 141037 usec (overhead 4 usec + queue 84557 usec + compute input 10 usec + compute infer 56454 usec + compute output 12 usec)\n",
      "\n",
      "Request concurrency: 6\n",
      "  Pass [1] throughput: 35.3333 infer/sec. Avg latency: 169268 usec (std 4135 usec)\n",
      "  Pass [2] throughput: 35.6667 infer/sec. Avg latency: 169265 usec (std 2271 usec)\n",
      "  Pass [3] throughput: 35.3333 infer/sec. Avg latency: 169413 usec (std 4038 usec)\n",
      "  Client: \n",
      "    Request count: 106\n",
      "    Throughput: 35.3333 infer/sec\n",
      "    Avg latency: 169413 usec (standard deviation 4038 usec)\n",
      "    p50 latency: 169246 usec\n",
      "    p90 latency: 175406 usec\n",
      "    p95 latency: 176906 usec\n",
      "    p99 latency: 177853 usec\n",
      "    Avg HTTP time: 169385 usec (send 5 usec + response wait 169379 usec + receive 1 usec)\n",
      "  Server: \n",
      "    Inference count: 127\n",
      "    Execution count: 127\n",
      "    Successful request count: 127\n",
      "    Avg request latency: 169009 usec (overhead 3 usec + queue 112634 usec + compute input 10 usec + compute infer 56347 usec + compute output 15 usec)\n",
      "\n",
      "Request concurrency: 7\n",
      "  Pass [1] throughput: 35.6667 infer/sec. Avg latency: 197749 usec (std 12387 usec)\n",
      "  Pass [2] throughput: 35.3333 infer/sec. Avg latency: 198030 usec (std 20767 usec)\n",
      "  Pass [3] throughput: 35.3333 infer/sec. Avg latency: 197883 usec (std 17477 usec)\n",
      "  Client: \n",
      "    Request count: 106\n",
      "    Throughput: 35.3333 infer/sec\n",
      "    Avg latency: 197883 usec (standard deviation 17477 usec)\n",
      "    p50 latency: 197756 usec\n",
      "    p90 latency: 222022 usec\n",
      "    p95 latency: 222658 usec\n",
      "    p99 latency: 223588 usec\n",
      "    Avg HTTP time: 197952 usec (send 6 usec + response wait 197944 usec + receive 2 usec)\n",
      "  Server: \n",
      "    Inference count: 127\n",
      "    Execution count: 127\n",
      "    Successful request count: 127\n",
      "    Avg request latency: 197562 usec (overhead 2 usec + queue 141059 usec + compute input 11 usec + compute infer 56478 usec + compute output 12 usec)\n",
      "\n",
      "Request concurrency: 8\n",
      "  Pass [1] throughput: 35.3333 infer/sec. Avg latency: 226027 usec (std 4113 usec)\n",
      "  Pass [2] throughput: 35.3333 infer/sec. Avg latency: 226457 usec (std 3320 usec)\n",
      "  Pass [3] throughput: 35 infer/sec. Avg latency: 226581 usec (std 3423 usec)\n",
      "  Client: \n",
      "    Request count: 105\n",
      "    Throughput: 35 infer/sec\n",
      "    Avg latency: 226581 usec (standard deviation 3423 usec)\n",
      "    p50 latency: 226389 usec\n",
      "    p90 latency: 231543 usec\n",
      "    p95 latency: 232401 usec\n",
      "    p99 latency: 233681 usec\n",
      "    Avg HTTP time: 226538 usec (send 6 usec + response wait 226530 usec + receive 2 usec)\n",
      "  Server: \n",
      "    Inference count: 127\n",
      "    Execution count: 127\n",
      "    Successful request count: 127\n",
      "    Avg request latency: 226153 usec (overhead 2 usec + queue 169581 usec + compute input 10 usec + compute infer 56548 usec + compute output 12 usec)\n",
      "\n",
      "Request concurrency: 9\n",
      "  Pass [1] throughput: 35.6667 infer/sec. Avg latency: 254114 usec (std 9236 usec)\n",
      "  Pass [2] throughput: 35 infer/sec. Avg latency: 255380 usec (std 5878 usec)\n",
      "  Pass [3] throughput: 35.3333 infer/sec. Avg latency: 255079 usec (std 16878 usec)\n",
      "  Client: \n",
      "    Request count: 106\n",
      "    Throughput: 35.3333 infer/sec\n",
      "    Avg latency: 255079 usec (standard deviation 16878 usec)\n",
      "    p50 latency: 256564 usec\n",
      "    p90 latency: 275943 usec\n",
      "    p95 latency: 276484 usec\n",
      "    p99 latency: 278020 usec\n",
      "    Avg HTTP time: 255227 usec (send 6 usec + response wait 255220 usec + receive 1 usec)\n",
      "  Server: \n",
      "    Inference count: 127\n",
      "    Execution count: 127\n",
      "    Successful request count: 127\n",
      "    Avg request latency: 254843 usec (overhead 3 usec + queue 198228 usec + compute input 12 usec + compute infer 56590 usec + compute output 10 usec)\n",
      "\n",
      "Request concurrency: 10\n",
      "  Pass [1] throughput: 35.3333 infer/sec. Avg latency: 282433 usec (std 6851 usec)\n",
      "  Pass [2] throughput: 35 infer/sec. Avg latency: 283967 usec (std 4263 usec)\n",
      "  Pass [3] throughput: 35.3333 infer/sec. Avg latency: 284751 usec (std 11065 usec)\n",
      "  Client: \n",
      "    Request count: 106\n",
      "    Throughput: 35.3333 infer/sec\n",
      "    Avg latency: 284751 usec (standard deviation 11065 usec)\n",
      "    p50 latency: 284181 usec\n",
      "    p90 latency: 300887 usec\n",
      "    p95 latency: 302718 usec\n",
      "    p99 latency: 305588 usec\n",
      "    Avg HTTP time: 284621 usec (send 8 usec + response wait 284611 usec + receive 2 usec)\n",
      "  Server: \n",
      "    Inference count: 126\n",
      "    Execution count: 126\n",
      "    Successful request count: 126\n",
      "    Avg request latency: 284183 usec (overhead 4 usec + queue 227385 usec + compute input 17 usec + compute infer 56758 usec + compute output 19 usec)\n",
      "\n",
      "Inferences/Second vs. Client Average Batch Latency\n",
      "Concurrency: 1, throughput: 35.3333 infer/sec, latency 28263 usec\n",
      "Concurrency: 2, throughput: 35.6667 infer/sec, latency 56062 usec\n",
      "Concurrency: 3, throughput: 35.6667 infer/sec, latency 84408 usec\n",
      "Concurrency: 4, throughput: 35.3333 infer/sec, latency 112552 usec\n",
      "Concurrency: 5, throughput: 35 infer/sec, latency 141452 usec\n",
      "Concurrency: 6, throughput: 35.3333 infer/sec, latency 169413 usec\n",
      "Concurrency: 7, throughput: 35.3333 infer/sec, latency 197883 usec\n",
      "Concurrency: 8, throughput: 35 infer/sec, latency 226581 usec\n",
      "Concurrency: 9, throughput: 35.3333 infer/sec, latency 255079 usec\n",
      "Concurrency: 10, throughput: 35.3333 infer/sec, latency 284751 usec\n"
     ]
    }
   ],
   "source": [
    "maxConcurrency= \"10\"\n",
    "batchSize=\"1\"\n",
    "print(\"Running: \" + modelName)\n",
    "!bash ./utilities/run_perf_client_local.sh \\\n",
    "                    {modelName} \\\n",
    "                    {modelVersion} \\\n",
    "                    {precision} \\\n",
    "                    {batchSize} \\\n",
    "                    {maxLatency} \\\n",
    "                    {maxClientThreads} \\\n",
    "                    {maxConcurrency} \\\n",
    "                    {tritonServerHostName} \\\n",
    "                    {dockerBridge} \\\n",
    "                    {resultsFolderName} \\\n",
    "                    {profilingData}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we continue, let's free up some GPU memory by moving some of the models out of the Triton model repository.  After removing the following three models, only the `bertQA-torchscript` model should remain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bertQA-torchscript\n"
     ]
    }
   ],
   "source": [
    "# Remove models from the inference server by removing them from the model_repository\n",
    "!mv /dli/task/model_repository/bertQA-onnx /dli/task/candidatemodels/\n",
    "!mv /dli/task/model_repository/bertQA-onnx-conexec /dli/task/candidatemodels/\n",
    "!mv /dli/task/model_repository/bertQA-onnx-trt-fp16 /dli/task/candidatemodels/\n",
    "\n",
    "# List remaining models on the inference server\n",
    "!ls /dli/task/model_repository"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Scheduling Strategies\n",
    "Triton supports batch inferencing by allowing individual inference requests to specify a batch of inputs. The inferencing for a batch of inputs is performed at the same time which is especially important for GPUs since it can greatly increase inferencing throughput. In many use cases the individual inference requests are not batched, therefore, they do not benefit from the throughput benefits of batching. <br/>\n",
    "\n",
    "The inference server contains multiple scheduling and batching algorithms that support many different model types and use-cases. The choice of the scheduler / batcher will be driven by several factors the key ones being:\n",
    "- Stateful / stateless nature of your inference workload\n",
    "- Whether your application is composed of models served in isolation or whether a more complex pipeline / ensemble is being used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.1 Stateless Inference\n",
    "\n",
    "When dealing with stateless inference (as we are in this class) we have two main options when it comes to scheduling. The first option is the default scheduler which will distribute request to all instances assigned for inference. This is the preferred option when the structure of the inference workload is well understood and where inference will take place at regular batch sizes and time intervals.\n",
    "\n",
    "The second option is dynamic batching which combines individual request and similarly to the default batcher distributes the larges batches across instances. We will discuss this particular option in the next section of the class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.2 Stateful Inference\n",
    "\n",
    "A stateful model (or stateful custom backend) does maintain state between inference requests. The model is expecting multiple inference requests that together form a sequence of inferences that must be routed to the same model instance so that the state being maintained by the model is correctly updated. Moreover, the model may require that Triton provide control signals indicating, for example, sequence start.\n",
    "\n",
    "The sequence batcher can employ one of two scheduling strategies when deciding how to batch the sequences that are routed to the same model instance. These strategies are Direct and Oldest.\n",
    "\n",
    "With the Direct scheduling strategy the sequence batcher ensures not only that all inference requests in a sequence are routed to the same model instance, but also that each sequence is routed to a dedicated batch slot within the model instance. This strategy is required when the model maintains state for each batch slot, and is expecting all inference requests for a given sequence to be routed to the same slot so that the state is correctly updated.\n",
    "\n",
    "With the Oldest scheduling strategy the sequence batcher ensures that all inference requests in a sequence are routed to the same model instance and then uses the dynamic batcher to batch together multiple inferences from different sequences into a batch that inferences together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.3 Pipelines / Ensembles\n",
    "\n",
    "An ensemble model represents a pipeline of one or more models and the connection of input and output tensors between those models. Ensemble models are intended to be used to encapsulate a procedure that involves multiple models, such as \"data preprocessing -> inference -> data post-processing\". Using ensemble models for this purpose can avoid the overhead of transferring intermediate tensors and minimize the number of requests that must be sent to Triton. An example of an ensemble pipeline is illustrated below: <br/>\n",
    "\n",
    "<img src=\"images/ensemble_example0.png\"/>\n",
    "\n",
    "The ensemble scheduler must be used for ensemble models, regardless of the scheduler used by the models within the ensemble. With respect to the ensemble scheduler, an ensemble model is not an actual model. Instead, it specifies the data flow between models within the ensemble as Step. The scheduler collects the output tensors in each step, provides them as input tensors for other steps according to the specification. In spite of that, the ensemble model is still viewed as a single model from an external view.\n",
    "\n",
    "More information on Triton scheduling can be found in the <a href=\"https://docs.nvidia.com/deeplearning/triton-inference-server/master-user-guide/docs/models_and_schedulers.html#stateless-models\">following section of the documentation</a>. In this class, we will focus further on one of the most powerful features of Triton, *dynamic batching*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 Dynamic Batching\n",
    "Dynamic batching is a feature of Triton that allows inference requests to be combined by the server, so that a batch is created dynamically, resulting in increased throughput.\n",
    "\n",
    "When a model instance becomes available for inferencing, the dynamic batcher will attempt to create batches from the requests that are available in the scheduler. Requests are added to the batch in the order the requests were received. If the dynamic batcher can form a batch of a preferred size(s) it will create a batch of the largest possible preferred size and send it for inferencing. If the dynamic batcher cannot form a batch of a preferred size, it will send a batch of the largest size possible that is less than the max batch size allowed by the model. \n",
    "\n",
    "The dynamic batcher can be configured to allow requests to be delayed for a limited time in the scheduler to allow other requests to join the dynamic batch. For example, the following configuration sets the maximum delay time of 100 microseconds for a request:\n",
    "\n",
    "```\n",
    "dynamic_batching {\n",
    "  preferred_batch_size: [ 4, 8 ]\n",
    "  max_queue_delay_microseconds: 100\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.1 Exercise: Implement Dynamic Batching\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin again by exporting an ONNX model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelName = \"bertQA-onnx-trt-dynbatch\"\n",
    "exportFormat = \"onnx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deploying model bertQA-onnx-trt-dynbatch in format onnxruntime_onnx\n",
      "/opt/conda/lib/python3.6/site-packages/torch/onnx/utils.py:955: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input input__0\n",
      "  'Automatically generated names will be applied to each dynamic axes of input {}'.format(key))\n",
      "/opt/conda/lib/python3.6/site-packages/torch/onnx/utils.py:955: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input input__1\n",
      "  'Automatically generated names will be applied to each dynamic axes of input {}'.format(key))\n",
      "/opt/conda/lib/python3.6/site-packages/torch/onnx/utils.py:955: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input input__2\n",
      "  'Automatically generated names will be applied to each dynamic axes of input {}'.format(key))\n",
      "/opt/conda/lib/python3.6/site-packages/torch/onnx/utils.py:955: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input output__0\n",
      "  'Automatically generated names will be applied to each dynamic axes of input {}'.format(key))\n",
      "/opt/conda/lib/python3.6/site-packages/torch/onnx/utils.py:955: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input output__1\n",
      "  'Automatically generated names will be applied to each dynamic axes of input {}'.format(key))\n",
      "[libprotobuf WARNING google/protobuf/io/coded_stream.cc:604] Reading dangerously large protocol message.  If the message turns out to be larger than 2147483647 bytes, parsing will be halted for security reasons.  To increase the limit (or to disable these warnings), see CodedInputStream::SetTotalBytesLimit() in google/protobuf/io/coded_stream.h.\n",
      "[libprotobuf WARNING google/protobuf/io/coded_stream.cc:81] The total number of bytes read was 1336539973\n",
      "\n",
      "conversion correctness test results\n",
      "-----------------------------------\n",
      "maximal absolute error over dataset (L_inf):  0.00022935867309570312\n",
      "\n",
      "average L_inf error over output tensors:  0.0001423954963684082\n",
      "variance of L_inf error over output tensors:  6.657553323445124e-09\n",
      "stddev of L_inf error over output tensors:  8.159383140559784e-05\n",
      "\n",
      "time of error check of native model:  0.4246840476989746 seconds\n",
      "time of error check of onnx model:  22.538012266159058 seconds\n",
      "\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "!python ./deployer/deployer.py \\\n",
    "    --{exportFormat} \\\n",
    "    --save-dir ./candidatemodels \\\n",
    "    --triton-model-name {modelName} \\\n",
    "    --triton-model-version 1 \\\n",
    "    --triton-max-batch-size 8 \\\n",
    "    --triton-dyn-batching-delay 0 \\\n",
    "    --triton-engine-count 1 \\\n",
    "    -- --checkpoint ./data/bert_qa.pt \\\n",
    "    --config_file ./bert_config.json \\\n",
    "    --vocab_file ./vocab \\\n",
    "    --predict_file ./squad/v1.1/dev-v1.1.json \\\n",
    "    --do_lower_case \\\n",
    "    --batch_size=8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise Steps\n",
    "1. Modify [config.pbtxt](candidatemodels/bertQA-onnx-trt-dynbatch/config.pbtxt) for dynamic batching using the example snippet. \n",
    "\n",
    "    ```\n",
    "    dynamic_batching {\n",
    "      preferred_batch_size: [ 4, 8 ]\n",
    "      max_queue_delay_microseconds: 100\n",
    "    }\n",
    "    ```\n",
    "    \n",
    "2. Enable TensorRT in the optimization block.\n",
    "\n",
    "    ```\n",
    "    optimization {\n",
    "       execution_accelerators {\n",
    "          gpu_execution_accelerator : [ {\n",
    "             name : \"tensorrt\"\n",
    "             parameters { key: \"precision_mode\" value: \"FP16\" }\n",
    "          }]\n",
    "       }\n",
    "    cuda { graphs: 0 }\n",
    "    }\n",
    "    ```\n",
    "3. Once saved, move the model to the Triton model repository and run the performance utility by executing the following cells. ([solution](solutions/ex-2-3-1_config.pbtxt) if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config.pbxt code\n",
    "name: \"bertQA-onnx-trt-dynbatch\"\n",
    "platform: \"onnxruntime_onnx\"\n",
    "max_batch_size: 8\n",
    "input [\n",
    "{\n",
    "    name: \"input__0\"\n",
    "    data_type: TYPE_INT64\n",
    "    dims: [384]\n",
    "},\n",
    "{\n",
    "    name: \"input__1\"\n",
    "    data_type: TYPE_INT64\n",
    "    dims: [384]\n",
    "},\n",
    "{\n",
    "    name: \"input__2\"\n",
    "    data_type: TYPE_INT64\n",
    "    dims: [384]\n",
    "}\n",
    "]\n",
    "output [\n",
    "{\n",
    "    name: \"output__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [384]\n",
    "}, \n",
    "{\n",
    "    name: \"output__1\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [384]\n",
    "}\n",
    "]\n",
    "optimization {\n",
    "    execution_accelerators {\n",
    "       gpu_execution_accelerator : [ {\n",
    "          name : \"tensorrt\"\n",
    "          parameters { key: \"precision_mode\" value: \"FP16\" }\n",
    "       }]\n",
    "    }\n",
    "  cuda {\n",
    "    graphs: 0\n",
    "  }\n",
    "}\n",
    "instance_group [\n",
    "    {\n",
    "        count: 1\n",
    "        kind: KIND_GPU\n",
    "        gpus: [ 0 ]\n",
    "    }\n",
    "]\n",
    "dynamic_batching {\n",
    "  preferred_batch_size: [ 4, 8 ]\n",
    "  max_queue_delay_microseconds: 100\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv ./candidatemodels/bertQA-onnx-trt-dynbatch model_repository/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: bertQA-onnx-trt-dynbatch\n",
      "Waiting for Triton Server to be ready at triton:8000...\n",
      "200\n",
      "......................Triton Server is ready!\n",
      "WARNING: Overriding max_threads specification to ensure requested concurrency range.\n",
      "*** Measurement Settings ***\n",
      "  Batch size: 1\n",
      "  Measurement window: 3000 msec\n",
      "  Latency limit: 500 msec\n",
      "  Concurrency limit: 10 concurrent requests\n",
      "  Using synchronous calls for inference\n",
      "  Stabilizing using average latency\n",
      "\n",
      "Request concurrency: 1\n",
      "  Pass [1] throughput: 35.3333 infer/sec. Avg latency: 28296 usec (std 142 usec)\n",
      "  Pass [2] throughput: 35.3333 infer/sec. Avg latency: 28400 usec (std 119 usec)\n",
      "  Pass [3] throughput: 35 infer/sec. Avg latency: 28550 usec (std 175 usec)\n",
      "  Client: \n",
      "    Request count: 105\n",
      "    Throughput: 35 infer/sec\n",
      "    Avg latency: 28550 usec (standard deviation 175 usec)\n",
      "    p50 latency: 28527 usec\n",
      "    p90 latency: 28802 usec\n",
      "    p95 latency: 28837 usec\n",
      "    p99 latency: 29061 usec\n",
      "    Avg HTTP time: 28533 usec (send 7 usec + response wait 28524 usec + receive 2 usec)\n",
      "  Server: \n",
      "    Inference count: 126\n",
      "    Execution count: 126\n",
      "    Successful request count: 126\n",
      "    Avg request latency: 28157 usec (overhead 4 usec + queue 183 usec + compute input 13 usec + compute infer 27939 usec + compute output 18 usec)\n",
      "\n",
      "Request concurrency: 2\n",
      "  Pass [1] throughput: 35.6667 infer/sec. Avg latency: 56127 usec (std 214 usec)\n",
      "  Pass [2] throughput: 35.3333 infer/sec. Avg latency: 56168 usec (std 218 usec)\n",
      "  Pass [3] throughput: 35.6667 infer/sec. Avg latency: 56144 usec (std 212 usec)\n",
      "  Client: \n",
      "    Request count: 107\n",
      "    Throughput: 35.6667 infer/sec\n",
      "    Avg latency: 56144 usec (standard deviation 212 usec)\n",
      "    p50 latency: 56170 usec\n",
      "    p90 latency: 56375 usec\n",
      "    p95 latency: 56498 usec\n",
      "    p99 latency: 56666 usec\n",
      "    Avg HTTP time: 56136 usec (send 5 usec + response wait 56129 usec + receive 2 usec)\n",
      "  Server: \n",
      "    Inference count: 129\n",
      "    Execution count: 129\n",
      "    Successful request count: 129\n",
      "    Avg request latency: 55811 usec (overhead 3 usec + queue 27821 usec + compute input 9 usec + compute infer 27966 usec + compute output 12 usec)\n",
      "\n",
      "Request concurrency: 3\n",
      "  Pass [1] throughput: 53.6667 infer/sec. Avg latency: 56319 usec (std 257 usec)\n",
      "  Pass [2] throughput: 53 infer/sec. Avg latency: 56420 usec (std 223 usec)\n",
      "  Pass [3] throughput: 53 infer/sec. Avg latency: 56249 usec (std 194 usec)\n",
      "  Client: \n",
      "    Request count: 159\n",
      "    Throughput: 53 infer/sec\n",
      "    Avg latency: 56249 usec (standard deviation 194 usec)\n",
      "    p50 latency: 56256 usec\n",
      "    p90 latency: 56468 usec\n",
      "    p95 latency: 56600 usec\n",
      "    p99 latency: 56649 usec\n",
      "    Avg HTTP time: 56279 usec (send 6 usec + response wait 56272 usec + receive 1 usec)\n",
      "  Server: \n",
      "    Inference count: 192\n",
      "    Execution count: 128\n",
      "    Successful request count: 128\n",
      "    Avg request latency: 55889 usec (overhead 3 usec + queue 27844 usec + compute input 10 usec + compute infer 28018 usec + compute output 14 usec)\n",
      "\n",
      "Request concurrency: 4\n",
      "  Pass [1] throughput: 71.3333 infer/sec. Avg latency: 56530 usec (std 247 usec)\n",
      "  Pass [2] throughput: 70.6667 infer/sec. Avg latency: 56455 usec (std 234 usec)\n",
      "  Pass [3] throughput: 70 infer/sec. Avg latency: 56612 usec (std 680 usec)\n",
      "  Client: \n",
      "    Request count: 210\n",
      "    Throughput: 70 infer/sec\n",
      "    Avg latency: 56612 usec (standard deviation 680 usec)\n",
      "    p50 latency: 56495 usec\n",
      "    p90 latency: 56901 usec\n",
      "    p95 latency: 57088 usec\n",
      "    p99 latency: 61089 usec\n",
      "    Avg HTTP time: 56589 usec (send 5 usec + response wait 56583 usec + receive 1 usec)\n",
      "  Server: \n",
      "    Inference count: 254\n",
      "    Execution count: 127\n",
      "    Successful request count: 127\n",
      "    Avg request latency: 56189 usec (overhead 4 usec + queue 28012 usec + compute input 10 usec + compute infer 28150 usec + compute output 13 usec)\n",
      "\n",
      "Request concurrency: 5\n",
      "  Pass [1] throughput: 87.3333 infer/sec. Avg latency: 56665 usec (std 306 usec)\n",
      "  Pass [2] throughput: 88.3333 infer/sec. Avg latency: 56616 usec (std 257 usec)\n",
      "  Pass [3] throughput: 87.6667 infer/sec. Avg latency: 56696 usec (std 237 usec)\n",
      "  Client: \n",
      "    Request count: 263\n",
      "    Throughput: 87.6667 infer/sec\n",
      "    Avg latency: 56696 usec (standard deviation 237 usec)\n",
      "    p50 latency: 56662 usec\n",
      "    p90 latency: 57040 usec\n",
      "    p95 latency: 57114 usec\n",
      "    p99 latency: 57293 usec\n",
      "    Avg HTTP time: 56691 usec (send 6 usec + response wait 56683 usec + receive 2 usec)\n",
      "  Server: \n",
      "    Inference count: 318\n",
      "    Execution count: 127\n",
      "    Successful request count: 127\n",
      "    Avg request latency: 56245 usec (overhead 3 usec + queue 28030 usec + compute input 12 usec + compute infer 28185 usec + compute output 15 usec)\n",
      "\n",
      "Request concurrency: 6\n",
      "  Pass [1] throughput: 106 infer/sec. Avg latency: 56752 usec (std 276 usec)\n",
      "  Pass [2] throughput: 104.667 infer/sec. Avg latency: 56764 usec (std 272 usec)\n",
      "  Pass [3] throughput: 105.333 infer/sec. Avg latency: 56772 usec (std 261 usec)\n",
      "  Client: \n",
      "    Request count: 316\n",
      "    Throughput: 105.333 infer/sec\n",
      "    Avg latency: 56772 usec (standard deviation 261 usec)\n",
      "    p50 latency: 56769 usec\n",
      "    p90 latency: 57144 usec\n",
      "    p95 latency: 57215 usec\n",
      "    p99 latency: 57282 usec\n",
      "    Avg HTTP time: 56770 usec (send 6 usec + response wait 56762 usec + receive 2 usec)\n",
      "  Server: \n",
      "    Inference count: 382\n",
      "    Execution count: 127\n",
      "    Successful request count: 127\n",
      "    Avg request latency: 56283 usec (overhead 5 usec + queue 28045 usec + compute input 13 usec + compute infer 28202 usec + compute output 18 usec)\n",
      "\n",
      "Request concurrency: 7\n",
      "  Pass [1] throughput: 122.333 infer/sec. Avg latency: 56837 usec (std 274 usec)\n",
      "  Pass [2] throughput: 123.667 infer/sec. Avg latency: 56905 usec (std 239 usec)\n",
      "  Pass [3] throughput: 122.667 infer/sec. Avg latency: 57051 usec (std 337 usec)\n",
      "  Client: \n",
      "    Request count: 368\n",
      "    Throughput: 122.667 infer/sec\n",
      "    Avg latency: 57051 usec (standard deviation 337 usec)\n",
      "    p50 latency: 57049 usec\n",
      "    p90 latency: 57340 usec\n",
      "    p95 latency: 57442 usec\n",
      "    p99 latency: 58084 usec\n",
      "    Avg HTTP time: 57030 usec (send 7 usec + response wait 57021 usec + receive 2 usec)\n",
      "  Server: \n",
      "    Inference count: 445\n",
      "    Execution count: 127\n",
      "    Successful request count: 127\n",
      "    Avg request latency: 56457 usec (overhead 3 usec + queue 28128 usec + compute input 17 usec + compute infer 28285 usec + compute output 24 usec)\n",
      "\n",
      "Request concurrency: 8\n",
      "  Pass [1] throughput: 141.333 infer/sec. Avg latency: 56957 usec (std 214 usec)\n",
      "  Pass [2] throughput: 141.333 infer/sec. Avg latency: 57014 usec (std 190 usec)\n",
      "  Pass [3] throughput: 141.333 infer/sec. Avg latency: 57070 usec (std 569 usec)\n",
      "  Client: \n",
      "    Request count: 424\n",
      "    Throughput: 141.333 infer/sec\n",
      "    Avg latency: 57070 usec (standard deviation 569 usec)\n",
      "    p50 latency: 56994 usec\n",
      "    p90 latency: 57251 usec\n",
      "    p95 latency: 57368 usec\n",
      "    p99 latency: 60796 usec\n",
      "    Avg HTTP time: 57047 usec (send 6 usec + response wait 57040 usec + receive 1 usec)\n",
      "  Server: \n",
      "    Inference count: 504\n",
      "    Execution count: 126\n",
      "    Successful request count: 126\n",
      "    Avg request latency: 56529 usec (overhead 5 usec + queue 28180 usec + compute input 14 usec + compute infer 28311 usec + compute output 19 usec)\n",
      "\n",
      "Request concurrency: 9\n",
      "  Pass [1] throughput: 140 infer/sec. Avg latency: 64139 usec (std 12301 usec)\n",
      "  Pass [2] throughput: 140 infer/sec. Avg latency: 64327 usec (std 12326 usec)\n",
      "  Pass [3] throughput: 140 infer/sec. Avg latency: 64314 usec (std 12330 usec)\n",
      "  Client: \n",
      "    Request count: 420\n",
      "    Throughput: 140 infer/sec\n",
      "    Avg latency: 64314 usec (standard deviation 12330 usec)\n",
      "    p50 latency: 57263 usec\n",
      "    p90 latency: 85720 usec\n",
      "    p95 latency: 85891 usec\n",
      "    p99 latency: 86090 usec\n",
      "    Avg HTTP time: 64297 usec (send 6 usec + response wait 64290 usec + receive 1 usec)\n",
      "  Server: \n",
      "    Inference count: 504\n",
      "    Execution count: 126\n",
      "    Successful request count: 126\n",
      "    Avg request latency: 63778 usec (overhead 4 usec + queue 35377 usec + compute input 14 usec + compute infer 28364 usec + compute output 19 usec)\n",
      "\n",
      "Request concurrency: 10\n",
      "  Pass [1] throughput: 140 infer/sec. Avg latency: 71597 usec (std 14254 usec)\n",
      "  Pass [2] throughput: 138.667 infer/sec. Avg latency: 71611 usec (std 14213 usec)\n",
      "  Pass [3] throughput: 140 infer/sec. Avg latency: 71511 usec (std 14233 usec)\n",
      "  Client: \n",
      "    Request count: 420\n",
      "    Throughput: 140 infer/sec\n",
      "    Avg latency: 71511 usec (standard deviation 14233 usec)\n",
      "    p50 latency: 84971 usec\n",
      "    p90 latency: 85919 usec\n",
      "    p95 latency: 86095 usec\n",
      "    p99 latency: 86419 usec\n",
      "    Avg HTTP time: 71506 usec (send 6 usec + response wait 71499 usec + receive 1 usec)\n",
      "  Server: \n",
      "    Inference count: 504\n",
      "    Execution count: 126\n",
      "    Successful request count: 126\n",
      "    Avg request latency: 70967 usec (overhead 5 usec + queue 42573 usec + compute input 14 usec + compute infer 28353 usec + compute output 22 usec)\n",
      "\n",
      "Inferences/Second vs. Client Average Batch Latency\n",
      "Concurrency: 1, throughput: 35 infer/sec, latency 28550 usec\n",
      "Concurrency: 2, throughput: 35.6667 infer/sec, latency 56144 usec\n",
      "Concurrency: 3, throughput: 53 infer/sec, latency 56249 usec\n",
      "Concurrency: 4, throughput: 70 infer/sec, latency 56612 usec\n",
      "Concurrency: 5, throughput: 87.6667 infer/sec, latency 56696 usec\n",
      "Concurrency: 6, throughput: 105.333 infer/sec, latency 56772 usec\n",
      "Concurrency: 7, throughput: 122.667 infer/sec, latency 57051 usec\n",
      "Concurrency: 8, throughput: 141.333 infer/sec, latency 57070 usec\n",
      "Concurrency: 9, throughput: 140 infer/sec, latency 64314 usec\n",
      "Concurrency: 10, throughput: 140 infer/sec, latency 71511 usec\n"
     ]
    }
   ],
   "source": [
    "modelName = \"bertQA-onnx-trt-dynbatch\"\n",
    "maxConcurency= \"10\"\n",
    "batchSize=\"1\"\n",
    "print(\"Running: \"+modelName)\n",
    "!bash ./utilities/run_perf_client_local.sh \\\n",
    "                    {modelName} \\\n",
    "                    {modelVersion} \\\n",
    "                    {precision} \\\n",
    "                    {batchSize} \\\n",
    "                    {maxLatency} \\\n",
    "                    {maxClientThreads} \\\n",
    "                    {maxConcurency} \\\n",
    "                    {tritonServerHostName} \\\n",
    "                    {dockerBridge} \\\n",
    "                    {resultsFolderName} \\\n",
    "                    {profilingData}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should have observed a fairly dramatic improvement in both latency and throughput. \n",
    "* How big is the impact in comparison to vanilla ONNX configuration or vanilla TorchScript? \n",
    "* What do you think was bottlenecking the multiple instance implementation?\n",
    "\n",
    "Discuss the results with the instructor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:green;\">Congratulations!</h3><br>\n",
    "You've leaned some strategies to improve the GPU utilization and reduce latency using:\n",
    "\n",
    "* Concurrent model execution\n",
    "* Scheduling\n",
    "* Dynamic batching\n",
    "\n",
    "In the next segment of the class we will make a more formal assessment of inference performance across multiple concurrency levels and how to analyze your inference performance in a structured way. Please proceed to the next notebook:<br>\n",
    "[3.0 Server Performance](030_ServerPerformance.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
